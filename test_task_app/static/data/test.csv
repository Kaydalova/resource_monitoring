topic_id;title;question;answer
8;;Разница continuous deployment vs continuous delivery;CI (непрерывная интеграция) — это способ интеграции изменений кода в репозиторий по несколько раз в день. У CD есть два значения: непрерывная доставка автоматизирует интеграцию в то время, как непрерывное развертывание автоматически выпускает финальную сборку для конечных пользователей. Регулярное тестирование в рамках CI/CD уменьшает количество ошибок и дефектов кода
1;Async в Python;Конкурентность и параллелизм в Python;"В случае конкурентность несколько задач работают в течение одного промежутка времени, но только одна активна в каждый момент. Задачи при этом как бы ""конкурируют"" за исполнителя - ядро CPU. Конкурентность можно организовать имея процессор всего с одним ядром, применив вытесняющую многозадачность для переключения между задачами.

Говоря о параллельной работе, мы имеем ввиду, что две или более задачи не просто чередуются, а выполняются строго в одно и то же время. На машине с одним ядром это невозможно, необходимо иметь процессор с несколькими ядрами."
1;Async в Python;Потоки и процессы в Python;"Процессом называется работающее приложение, которому выделена область памяти, недоступная другим приложениям. Если машина оснащена процессором с несколькими ядрами - то несколько процессов могут работать одновременно. Если процессор имеет только одно ядро - то все равно можно выполнять несколько приложений конкурентно, но с приминением квантования времени. При использовании квантования операционная система будет автоматически вытеснять работающий процесс по истечении некоторого промежутка времени и передавать процессор другому процессу. Алгоритмы, определяющие, в какой момент переключать процессы зависят от ОС.

У потоков нет своей памяти, они пользуются памятью создавшего их процесса. Потоки ассоциированны с процессом, создавшим их. С каждым процессом всегда ассоциирован по меньшей мере один поток, обычно называемый главным. Процесс может создавать дополнительные потоки, которые обычно называются рабочими или фоновыми, они могут выполнять другую работу, это называется многопоточностью."
1;Async в Python;Расскажите, что такое асинхронный код, и приведите пример.;"Асинхронный код - это код, имеющий возможность переключаться между своими асинхронными функциями. Делает он это не произвольно в любом месте в любое время, а при помощи асинхронного цикла событий, переключаещегося на выполнение других задач в тех местах нашего кода, где существуют временные задержки, например операции ввода-вывода.
При выполнении такого кода может создаваться ощущение, что асинхронные функции выполняются одновременно и ""параллельно"", но это не так.

Ещё раз повторюсь: это неканоничное определение ""своими словами"" и немного не с той стороны. Со стороны процессора.

Возьмём одно ядро процессора и запустим на нем какой-то осмысленный синхронный код, который выполняет какие-то полезные операции, например, делает http-запросы к сайтам, чтобы их распарсить. Пока происходит магия отправки запроса через сеть куда-то там очень далеко к веб-серверу и получение ответа на запрос, проходит время - какие-то миллисекунды, сотни миллисекунд. Всё это время программа больше ничем не занята, ядро процессора или не занято ничем, или операционная система использует его по своему усмотрению (мы об этом не знаем и не думаем).

Если построить график нагрузки нашего процессорного ядра, то он будет напоминать расчёску - тут работаем, тут ждём, тут снова работаем.

Мы могли бы запустить ещё один поток (thread), чтобы поплотнее занять наше ядро чем-нибудь полезным, но есть один недостаток: поток при выполнении не будет согласован с моментами тех самых блокирующих операций, пока мы его сами не согласуем: начнём использовать весь набор инструментов модуля threading - разные локи, семафоры и прочие инструменты, которыми ещё и нужно уметь пользоваться.

Вместо этого делают вот что. Функции-методы программы сами сообщают циклу событий ""сейчас будет долго, не жди меня, переключись куда-нибудь, что там у тебя дальше по списку"".

И цикл событий переключается на следующую асинхронную функцию, и так далее.
В итоге ядро процессора будет более плотно занято выполнением различных частей нашей программы и наши асинхронные функции сделают свою работу быстрее.

Пример из документации asyncio:

import asyncio

async def main():
‧‧‧‧print(&apos;Hello ...&apos;)
‧‧‧‧await asyncio.sleep(1)
‧‧‧‧print(&apos;... World!&apos;)

asyncio.run(main())


Как раз await явным образом указывает, что ""здесь можно переключиться"". Правда, некуда - функция в примере всего одна.

import asyncio


async def hello1():
‧‧‧‧print(&apos;Hello1 ...&apos;)
‧‧‧‧await asyncio.sleep(1)
‧‧‧‧print(&apos;... World1&apos;)


async def hello2():
‧‧‧‧print(&apos;Hello2 ...&apos;)
‧‧‧‧await asyncio.sleep(1)
‧‧‧‧print(&apos;... World2&apos;)


async def main():
‧‧‧‧await asyncio.gather(hello1(), hello2())


asyncio.run(main())


>>>Hello1 ...
>>>Hello2 ...
>>>... World1
>>>... World2

Что мы видим? После вывода текста Hello1 в функции hello1 цикл событий не стал ждать и переключился на функцию hello2, вывел Hello2 и вернулся в hello1, а потом завершил выполнение и hello2.

Документация сообщает нам, что ""asyncio is often a perfect fit for IO-bound and high-level structured network code"".

Это правда. Особенно когда нужно со стороны сервера как можно быстрее ответить на запросы по сети. Настолько быстро, насколько это вообще возможно (учитывая что Python ""медленный""). Мы асинхронно обрабатываем множество запросов, загружая почти полностью одно ядро процессора.
Хорошо, а если, мой бизнес говорит: у меня 12-ядерный Xeon, одно ядро у него делает полезную работу, а остальные простаивают, за что же уплачены деньги?
Всё просто. Запускаем 10 инстансов (запущенных копий) нашей программы (можно в docker, можно без), придумываем балансировщик нагрузки - зависит от того, какую задачу решаем, и вот уже нас высоконагруженный сервер. Ожидаем, что операционная система сама разумно распределит наши инстансы по ядрам."
1;Async в Python;Что такое многопоточность и асинхронность в Python? В чем разница между модулем threading и модулем asyncio?;"Многопоточность и асинхронность - это два подхода к параллельной и конкурентной обработке задач в Python, которые позволяют одновременно выполнять несколько операций или задач, улучшая производительность и отзывчивость приложений.

Многопоточность (threading): Многопоточность - это подход, при котором одновременно выполняются несколько потоков выполнения в рамках одного процесса. В Python модуль threading предоставляет функциональность для создания и управления потоками. Важно отметить, что из-за Global Interpreter Lock (GIL) в стандартной реализации Python (CPython) потоки не могут одновременно выполняться на нескольких процессорах, что ограничивает параллелизм на уровне инструкций. Однако многопоточность полезна для выполнения задач, ограниченных вводом-выводом (I/O-bound tasks), когда потоки могут выполняться в фоновом режиме и не блокировать другие потоки, ожидая завершения операций ввода-вывода.
Асинхронность (asyncio): Асинхронность - это подход, который использует один поток выполнения для обработки нескольких задач, переключаясь между ними при ожидании завершения асинхронных операций, таких как запросы к серверу или операции с файлами. Модуль asyncio в Python предоставляет функциональность для написания асинхронного кода с использованием сопрограмм (coroutines), асинхронных итераторов и контекстных менеджеров. Асинхронный подход эффективен для обработки большого количества I/O-bound задач с минимальными накладными расходами на переключение контекста и управление потоками.
Разница между модулем threading и модулем asyncio:

threading использует многопоточность, т.е. несколько потоков выполнения в рамках одного процесса, в то время как asyncio использует асинхронность, т.е. один поток выполнения с переключением между асинхронными задачами.
threading подходит для выполнения задач, которые блокируются из-за операций ввода-вывода, но его эффективность ограничена из-за GIL в CPython. asyncio эффективнее справляется с большим количеством I/O-bound задач, так как использует механизмы асинхронного ввода-вывода и переключения между задачами без блокирования выполнения.
threading может быть более простым для понимания и внедрения, особенно в случае с небольшим количеством потоков и простыми задачами. Однако asyncio требует более сложного управления состоянием и событиями, так как задачи должны явно указывать, когда они готовы отдать управление другим задачам.
В модуле threading потоки управляются операционной системой, и переключение контекста может происходить в любой момент времени, что может привести к состоянию гонки (race conditions) и другим проблемам с синхронизацией. В модуле asyncio переключение контекста происходит только в точках, где задачи явно передают управление другим задачам (обычно с использованием оператора await), что упрощает синхронизацию и предотвращение состояний гонки.
threading лучше подходит для CPU-bound задач, когда вы используете реализацию Python, которая не имеет GIL (например, Jython), или когда вы используете модуль multiprocessing, который позволяет обойти ограничения GIL. asyncio не является подходящим выбором для CPU-bound задач, так как одна вычислительно сложная задача может заблокировать выполнение всех остальных задач.
Давайте создадим пример, в котором мы хотим скачать несколько веб-страниц одновременно с использованием многопоточности (с модулем threading) и асинхронности (с модулем asyncio). Сначала создадим многопоточный вариант, а затем асинхронный, для сравнения.

Многопоточный вариант:

import requests
import threading
# Функция, которая будет скачивать веб-страницу
def download_page(url):
response = requests.get(url)
print(f""Завершено: {url} ({len(response.text)} символов)"")
# Список URL-адресов для скачивания
urls = [
""https://example.com"",
""https://www.python.org"",
""https://www.wikipedia.org""
]
# Создаем и запускаем потоки
threads = [threading.Thread(target=download_page, args=(url,)) for url in urls]
for thread in threads:
thread.start()
# Ожидаем завершения всех потоков
for thread in threads:
thread.join()
print(""Все страницы успешно скачаны."")
Асинхронный вариант:

import aiohttp
import asyncio
# Асинхронная функция, которая будет скачивать веб-страницу
async def download_page_async(url):
async with aiohttp.ClientSession() as session:
async with session.get(url) as response:
text = await response.text()
print(f""Завершено: {url} ({len(text)} символов)"")
# Список URL-адресов для скачивания
urls = [
""https://example.com"",
""https://www.python.org"",
""https://www.wikipedia.org""
]
# Создаем асинхронные задачи и запускаем их
loop = asyncio.get_event_loop()
tasks = [download_page_async(url) for url in urls]
loop.run_until_complete(asyncio.gather(*tasks))
print(""Все страницы успешно скачаны."")"
1;Async в Python;Потоки: как работают, причем тут GIL, где лучше использовать, как избежать проблем;"В Python есть несколько припятствий на пути организации конкурентности с помощью потоков. Многопоточность полезна только для IO-bound задач, потому что нам мешает GIL.

GIL не дает Python-процессу исполнять более одной команды байт-кода в каждый момент времени. Это означает, что даже если имеется несколько потоков на многоядерной машине, интерпретатор сможет в каждый момент времени исполнять только один поток, содержащий написанный на Python код. Таким образом если мы запустим CPU-bound задачи на нескольких потоках в рамках одного процесса, то в итоге они будут выполняться дольше однопоточного из-за накладных расходов на создание и управление потоками.

Для чего нужен GIL?
Мы помним, что потоки одного процесса имеют общую память. Если два потока будут одновременно модифицировать одну и ту же переменную, то ее конечное состояние может оказаться неожиданным (может сломаться счетчик ссылок).
Но GIL не удерживается постоянно, что открывает возможность использования нескольких потоков. Глобальная блокировка интерпретатора освобождается на время выполнения IO операций, а также time.sleep. Это позволяет использовать потоки для конкурентного выполнения ввода-вывода."
1;Comprehensions;Что такое comprehensions (list, generator, set, dictionary) в Python?;"В общем случае, list comprehension это выражение, которое преобразует итерируемый объект в список. То есть, последовательность элементов преобразуется и добавляется в новый список.
vlans = [int(vl) for vl in items]
Генераторы словарей аналогичны генераторам списков, но они используются для создания словарей.
d = {}
for num in range(1, 11):
d[num] = num**2
Генераторы множеств в целом аналогичны генераторам списков.
Например, надо получить множество с уникальными номерами VLAN’ов:
vlans = [10, &apos;30&apos;, 30, 10, &apos;56&apos;]
unique_vlans = {int(vlan) for vlan in vlans} # {10, 30, 56}"
1;Deep inside;Python работает, как Call-by-name или Call-by-reference? (Call-by-Sharing);"Python использует модель передачи параметров, называемую ""Call-by-Sharing"". Это означает, что при передаче аргументов в функцию, само значение аргумента не копируется, а передается ссылка на объект, с которым связан аргумент.

В случае изменяемых объектов (например, списков, словарей), изменения, сделанные внутри функции, могут отразиться на вызывающей стороне, поскольку обе стороны ссылаются на один и тот же объект. Однако, если функция присваивает новое значение аргументу, то это не будет отражено в вызывающей стороне, потому что это приведет к созданию новой ссылки на другой объект, а не изменит ссылку, переданную изначально.

В случае неизменяемых объектов (например, чисел, строк), любые изменения, сделанные внутри функции, не будут отражены на вызывающей стороне, потому что неизменяемые объекты не могут быть изменены."
1;Deep inside;Как работает механизм управления памятью в Python? Можете ли вы объяснить основные принципы сборки мусора и работы с памятью?;"Управление памятью в Python является автоматическим и основано на двух основных компонентах: менеджере памяти и сборщике мусора.

Менеджер памяти: Менеджер памяти Python отвечает за выделение и освобождение памяти. Он предоставляет API для выделения памяти под объекты, которые создаются в процессе выполнения программы. Менеджер памяти управляет пулом памяти, который выделяется при запуске интерпретатора. Когда объектам требуется больше памяти, менеджер памяти автоматически выделяет ее из пула.
Сборщик мусора (Garbage Collector, GC): Сборщик мусора в Python отвечает за автоматическое освобождение памяти, которую занимают объекты, которые больше не используются в программе. Основной алгоритм сборки мусора в Python основан на подсчете ссылок (reference counting). Каждый раз, когда объект ссылается на другой объект, счетчик ссылок этого объекта увеличивается. Когда ссылка на объект удаляется или выходит из области видимости, счетчик ссылок уменьшается. Если счетчик ссылок объекта становится равным нулю, объект считается мусором и его память освобождается.
Однако подсчет ссылок имеет ограничение в виде циклических ссылок, где два или более объектов ссылаются друг на друга, что делает невозможным уменьшение счетчика ссылок до нуля. Для решения этой проблемы Python использует дополнительный механизм сборки мусора, который периодически ищет и удаляет группы циклических ссылок.
Python также предоставляет возможность настройки сборки мусора и определения пользовательских методов очистки, таких как __del__() для управления ресурсами, которые не являются памятью, например, файлами или сетевыми соединениями.

Циклические ссылки:
import gc
# Создаем два объекта с циклическими ссылками
class MyClass:
def __init__(self):
self.reference = None
a = MyClass()
b = MyClass()
a.reference = b
b.reference = a
# Удаляем ссылки на объекты
del a
del b
# Принудительно запускаем сборку мусора
gc.collect()
В этом примере мы создаем два объекта класса MyClass, которые ссылаются друг на друга, создавая циклические ссылки. После удаления ссылок на объекты a и b счетчик ссылок на них не уменьшается до нуля из-за циклических ссылок. Однако сборщик мусора, основанный на циклах, сможет определить и удалить эти объекты. Мы принудительно вызываем сборку мусора с помощью gc.collect() для демонстрации.

Настройка сборки мусора:
import gc
# Отключаем сборку мусора
gc.disable()
# Выполняем какой-то код, который может создавать мусор
# ...
# Включаем сборку мусора
gc.enable()
# Принудительно запускаем сборку мусора
gc.collect()
В этом примере мы отключаем сборку мусора с помощью gc.disable(), выполняем какой-то код, который может создавать мусор, затем включаем сборку мусора с помощью gc.enable() и принудительно запускаем сборку мусора с помощью gc.collect(). Это может быть полезно для оптимизации производительности при выполнении критичных операций, где сборка мусора может негативно сказаться на производительности."
1;Deep inside;В чем разница между глубоким и поверхностным копированием? Каким образом можно осуществить глубокое копирование в Python?;"Глубокое и поверхностное копирование относятся к процессу создания копии объекта или структуры данных в Python.

Поверхностное копирование (shallow copy): При поверхностном копировании создается новый объект, который является копией исходного объекта, но вложенные объекты копируются только по ссылке. В результате, исходный объект и его копия будут ссылаться на одни и те же вложенные объекты. Это означает, что изменения во вложенных объектах копии также повлияют на исходный объект и наоборот.
Глубокое копирование (deep copy): Глубокое копирование создает новый объект и рекурсивно копирует все вложенные объекты. В результате получается полностью независимая копия исходного объекта со всеми вложенными объектами. Изменения в копии не влияют на исходный объект и наоборот.
Для выполнения глубокого копирования в Python используется модуль copy и его функция deepcopy(). Вот пример:

import copy
original_list = [1, 2, [3, 4], 5]
# Создаем глубокую копию
deep_copied_list = copy.deepcopy(original_list)
# Изменяем исходный список
original_list[2][0] = 99
# Выводим оба списка
print(""Original list:"", original_list)
print(""Deep copied list:"", deep_copied_list)
Вывод:

Original list: [1, 2, [99, 4], 5]
Deep copied list: [1, 2, [3, 4], 5]
В этом примере мы создаем глубокую копию списка original_list, изменяем исходный список и выводим оба списка. Изменения в исходном списке не повлияли на глубокую копию, потому что они являются полностью независимыми объектами со своими вложенными объектами."
1;Deep inside;Магический метод __repr__;"Он предназначен для того, чтобы правильно репрезентовать класс именно в том виде, как вы задумали. Без него print(nc) вывел бы что-то типа: &apos;<__main__.NewClass object at 0x7f06f7790460> &apos;"
1;Deep inside;Что хранится в атрибуте __dict__?;"А в этом атрибуте хранится всё, чем богат объект. Это внутренний, встроенный в объект словарь с именем и значением.
Именно такая внутренняя структура предусмотрена в механизме взаимодействия с атрибутами объекта. Когда мы обращаемся к object.someattr - на самом деле где-то там, внутри, интерпретатор идёт в object.__dict__[&apos;someattr&apos;] и проверяет, есть ли такой атрибут, и что в нём содержится.

class NewClass(object):
def __init__(self) -> None:
self.a = 1
print(self.__dict__)

def what(self) -> list:
""""""
Расскажем про все атрибуты
""""""
return list(self.__dict__.keys())


def __repr__(self) -> str:
""""""
Этот магический метод предназначен
для str представления нашего класса.
Это можно сделать при помощи """".join,
но так понятней и наглядней.
""""""
res = &apos;NewClass with attrs:&apos;
for key in list(self.__dict__.keys()):
res += f&apos;{key},&apos;
return res


if __name__ == &apos;__main__&apos;:
nc = NewClass()
nc.b = 2 # Нехорошо!
nc.__dict__[&apos;c&apos;] = 3 # Очень нехорошо!
print(f&apos;nc has attrs: {nc.what()}&apos;)
print(nc)
print(&apos;nc.__dict__ = &apos; + str(nc.__dict__))


>>>{&apos;a&apos;: 1}
>>>nc has attrs: [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]
>>>NewClass with attrs:a,b,c,
>>>nc.__dict__ = {&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3}

Что мы сделали? Сначала создали класс с атрибутом a, равным 1. Потом присвоили ему извне атрибут b=2, а потом ещё более изощрённым способом c=3. Так делать в жизни нельзя, изюм в булку лучше поместить ДО её запекания, а не заковыривать в уже готовый хлеб. Это было сделано только для примера, что мы так можем."
1;Deep inside;Как проверить файл .py на синтаксические ошибки, не запуская его?;"Скажу честно, я не знал ответа на этот вопрос. У меня есть PyFlakes, есть Yapf, есть Black, то есть всякие линтеры-форматтеры файлов, которые при синтаксической ошибке сообщают об этом.

Оказалось, есть и ещё несколько способов:

python3 -m compileall .
""Скомпилирует"" весь код в текущей папке в файлы .pyc

python3 -m py_compile script.py
Сделает то же самое с одним указанным файлом (что и было ""правильным"" ответом).

Не уверен, что это самый очевидный способ проверить файл на ошибки, но радует, что у Python всегда есть ""батарейки в комплекте""."
1;Deep inside;Какие пространства имен существуют в python?;"Пространство имен — это совокупность определенных в настоящий момент символических имен и информации об объектах, на которые они ссылаются.

Python имеет множество встроенных пространств имен. Некоторые из них включают:

builtins: содержит встроенные функции и типы, которые доступны в любой области видимости по умолчанию.

main: это специальное пространство имен, которое содержит определения, которые были выполнены на верхнем уровне скрипта или интерактивной оболочки Python.

name: это атрибут, который содержит имя текущего модуля. Если модуль импортирован, то значение name будет именем модуля. Если модуль запускается как скрипт, то значение name будет ""main"".

globals(): это функция, которая возвращает словарь, содержащий все имена в глобальной области видимости.

locals(): это функция, которая возвращает словарь, содержащий все имена в локальной области видимости.

Это далеко не полный список, но это некоторые из наиболее распространенных пространств имен в Python."
1;Deep inside;Что такое GIL? Почему GIL всё ещё существует?;"GIL (Global Interpreter Lock) - это механизм в интерпретаторе CPython , который гарантирует, что только один поток исполнения может выполнять байт-код Python в любой момент времени. Это было добавлено в Python для обеспечения безопасности потоков в многопоточной среде и для упрощения реализации интерпретатора.

GIL всё ещё существует, потому что он является важной частью интерпретатора CPython и его логики работы с потоками. Однако, недавние версии Python имеют некоторые механизмы для обхода ограничений GIL, такие как использование многопроцессных вычислений вместо многопоточных и использование асинхронного программирования. Кроме того, есть и другие реализации языка Python, такие как Jython и IronPython, которые не используют GIL.

Таким образом, вопрос насколько существование GIL ограничивает производительность Python в настоящее время является разногласием в сообществе."
1;Deep inside;"Что не так с этим кодом? Зачем это нужно?
if __debug__:
assert False, (""error"")";"В данном коде используется выражение if __debug__:, которое проверяет, выполняется ли код в режиме отладки. Затем следует утверждение assert False, которое всегда вызывает ошибку утверждения. Это означает, что если код выполняется в режиме отладки, то произойдет сбой программы с ошибкой ""AssertionError"" и сообщением ""error""."
1;Deep inside;Что такое monkey patching? Приведите пример использования.;"Monkey patching - это техника в программировании, при которой изменяется или расширяется поведение существующего кода во время выполнения программы путем динамического изменения или расширения его компонентов. Это позволяет добавлять, изменять или заменять методы, атрибуты или функциональность уже существующих классов или модулей без необходимости вносить изменения в исходный код.

Пример использования monkey patching:
Предположим, у нас есть класс Person с определенными методами и атрибутами:
class Person:
def __init__(self, name):
self.name = name

def greet(self):
print(f""Hello, my name is {self.name}!"")
Мы хотим добавить новый метод introduce, который будет выводить информацию о человеке:

def introduce(self):
print(f""My name is {self.name}. Nice to meet you!"")

person = Person(""Alice"")
person.introduce() # Ошибка: &apos;Person&apos; object has no attribute &apos;introduce&apos;
Вместо изменения исходного кода класса Person, мы можем использовать monkey patching для добавления нового метода:

def introduce(self):
print(f""My name is {self.name}. Nice to meet you!"")

Person.introduce = introduce
person = Person(""Alice"")
person.introduce() # Output: My name is Alice. Nice to meet you!
Таким образом, мы добавили новый метод introduce к классу Person путем присваивания функции introduce в качестве атрибута класса. Теперь объекты класса Person могут вызывать этот новый метод без изменения исходного кода класса."
1;Deep inside;Как работать с транзитивными зависимостями?;"Для работы с транзитивными зависимостями можно использовать систему управления зависимостями, например, pipenv, poetry или pip. Эти системы позволяют устанавливать зависимости и их транзитивные зависимости, а также контролировать версии зависимостей. Например, при использовании pipenv для установки и работы с зависимостями можно использовать следующие команды:

pipenv install <имя пакета>
Эта команда установит пакет и его транзитивные зависимости и создаст файл Pipfile с перечнем зависимостей и версиями.

pipenv shell
Эта команда позволит активировать виртуальное окружение, в котором установлены зависимости.

pipenv install --dev <имя пакета>
Эта команда установит пакет в качестве зависимости разработки.

pipenv uninstall <имя пакета>
Эта команда удалит пакет и его транзитивные зависимости.

Также можно использовать файлы requirements.txt или setup.py для установки зависимостей и их транзитивных зависимостей."
1;Deep inside;Как работает система импорта модулей в Python? В чем разница между абсолютным и относительным импортом? Каковы возможные подводные камни при использовании импорта?;"Система импорта модулей в Python позволяет загружать и использовать код из других модулей или пакетов. Она состоит из нескольких этапов:

Поиск модуля: Python ищет модуль или пакет в определенных местах, таких как текущий каталог, каталоги, указанные в переменной окружения PYTHONPATH, и стандартные каталоги библиотеки.
Компиляция модуля: Если модуль еще не загружен в память и найден в виде исходного кода (.py), Python компилирует его в байт-код (.pyc).
Загрузка и выполнение модуля: Python загружает скомпилированный модуль в память и выполняет его код.
Абсолютный и относительный импорт:

Абсолютный импорт: Абсолютный импорт опирается на полное имя модуля или пакета, начиная с корня иерархии пакетов. Это делает импорт более явным и понятным, так как путь к импортируемому модулю четко указан.
import mypackage.mymodule
from mypackage.mymodule import myfunction
Относительный импорт: Относительный импорт опирается на текущее местоположение модуля в иерархии пакетов. Он может быть удобен для компактного кода и облегчения рефакторинга, но может сделать импорт менее явным. Относительные импорты используют точки (.) для указания уровня относительности. Например:
from . import mymodule # Импорт из текущего пакета
from .. import mymodule # Импорт из родительского пакета
Подводные камни при использовании импорта:

Циклические импорты: Если два модуля импортируют друг друга, это может привести к циклическим зависимостям и проблемам с инициализацией. Чтобы избежать этого, можно использовать локальный импорт внутри функций, разбивать модули на более мелкие части или пересмотреть структуру пакетов.
Конфликты имен: Если два модуля с одинаковым именем импортируются из разных мест, это может привести к конфликтам имен и неожиданному поведению. Чтобы избежать этого, можно использовать псевдонимы (алиасы) с помощью ключевого слова as при импорте или структурировать пакеты и модули так, чтобы избегать дублирования имен.
Проблемы с поиском модулей: Если модуль не найден в указанных местах, Python выдаст ошибку ModuleNotFoundError. Чтобы избежать этого, убедитесь, что модули и пакеты находятся в правильных каталогах, и переменная окружения PYTHONPATH корректно настроена. Возможно, вам потребуется настроить виртуальное окружение или установить отсутствующие зависимости.
Проблемы с версиями: Если ваш код использует функции или атрибуты, которые доступны только в определенных версиях модулей или пакетов, импорт и использование этих модулей может вызвать ошибки или несовместимость. Чтобы избежать этого, убедитесь, что все зависимости имеют подходящие версии, и используйте инструменты, такие как pip и virtualenv, для управления зависимостями.
Проблемы с производительностью: Если модули содержат большое количество кода или требуют больших вычислительных ресурсов, импортирование и выполнение этих модулей может замедлить ваше приложение. Чтобы оптимизировать производительность, рассмотрите возможность импорта только тех частей модулей, которые действительно необходимы для работы вашего кода, и использования ленивых импортов, когда это возможно."
1;Генераторы;Что такое генератор и чем отличается от итератора?;"Можно сказать, что генератор является частным случаем итератора: он так же имеет методы __next__ и __iter__. Каждый генератор является итератором, но не наоборот.
Генератор использует для возврата значений ключевое слово yield.
Отличие в том, что генератор позволяет упростить код. Генератор не вычисляет и не содержит сразу все итерируемые значения, он генерирует их во время своей работы.
Пример генератора:
def fibonacci():
prev, cur = 0, 1
while True:
yield prev
prev, cur = cur, prev + cur
То есть, каждый элемент последовательности создается по очереди, и отсутствует заранее заготовленный итерируемый объект с данными.
Использование:
for i in fibonacci():
print(i)
if i > 100:
break"
1;Генераторы;Как перевернуть генератор?;"Можно перевернуть генератор в Python, используя функцию reversed(). Вот пример, который демонстрирует это:

my_list = [1, 2, 3, 4, 5]
my_generator = (x**2 for x in my_list)

for item in reversed(list(my_generator)):
print(item)
В этом примере мы используем функцию reversed() вместе с функцией list(), чтобы создать обратный список элементов, сгенерированных генератором. Затем мы используем этот список с циклом for для перебора элементов в обратном порядке. Если вы работаете с большими наборами данных, может быть полезно использовать обратное итерирование без использования list(), чтобы избежать создания полной копии. Вот пример, который демонстрирует это:

my_list = [1, 2, 3, 4, 5]
my_generator = (x**2 for x in my_list)

for item in reversed(tuple(my_generator)):
print(item)
Здесь мы используем функцию reversed() вместе с функцией tuple() для обратного итерирования через генератор без создания полной копии."
1;Генераторы;Для чего используется ключевое слово yield?;"Ключевое слово ""yield"" используется для создания генераторов. Генератор - это функция, которая может возвращать последовательность значений используя инструкции yield вместо return. При каждом вызове инструкции yield генератор возвращает значение, после чего сохраняет свое состояние и приостанавливает свое выполнение до следующего вызова. Это позволяет генерировать последовательности значений без необходимости создания и хранения всех значений в памяти, что может быть особенно полезно при работе с большими объемами данных. Кроме того, генераторы являются итерируемыми и могут использоваться в циклах for."
1;Декораторы;Как работают декораторы? Напиши декоратор;"Декораторы в Python - это функции высшего порядка, которые принимают одну функцию в качестве аргумента и возвращают другую функцию. Они позволяют расширять и модифицировать поведение функций или методов без изменения их кода. Декораторы часто используются для добавления функциональности, такой как логирование, кэширование, проверка доступа или измерение времени выполнения функций.

Пример декоратора:

def my_decorator(func):
def wrapper(*args, **kwargs):
print(""Действие перед выполнением функции"")
result = func(*args, **kwargs)
print(""Действие после выполнения функции"")
return result
return wrapper
@my_decorator
def my_function():
print(""Это моя функция"")
my_function()
В этом примере мы создаем декоратор my_decorator, который принимает функцию func в качестве аргумента. Внутри декоратора мы определяем новую функцию wrapper, которая вызывает функцию func и выполняет дополнительные действия до и после вызова. Затем мы применяем декоратор my_decorator к функции my_function с использованием синтаксиса @. При вызове my_function() будут выполняться действия, определенные в декораторе, до и после выполнения my_function.

Возможные применения декораторов:

Логирование: Декораторы могут использоваться для логирования информации о вызовах функций, такой как время вызова, аргументы и возвращаемые значения.
Измерение времени выполнения: Декораторы могут использоваться для измерения времени выполнения функций, что полезно при профилировании и оптимизации кода.
Кэширование: Декораторы могут использоваться для кэширования результатов функций, что особенно полезно при работе с ресурсоемкими или затратными операциями.
Проверка доступа: Декораторы могут использоваться для проверки прав доступа пользователя перед выполнением определенных функций или методов, обеспечивая безопасность приложения.
Валидация аргументов: Декораторы могут использоваться для проверки типов или значений аргументов функций, обеспечивая более строгий контроль над входными данными."
1;Задача;Напишите декоратор, который будет перехватывать ошибки и повторять функцию максимум N раз.;"import functools

def retry_on_error(max_attempts):
def decorator_retry(func):
@functools.wraps(func)
def wrapper_retry(*args, **kwargs):
attempts = 0
while attempts < max_attempts:
try:
return func(*args, **kwargs)
except Exception as e:
print(f""Attempt {attempts+1} failed with error: {str(e)}"")
attempts += 1
print(f""Reached maximum attempts ({max_attempts}). Giving up."")
return wrapper_retry
return decorator_retry"
1;Задача;Напиши программу, которая будет определять является ли строка палиндромом;"def is_palindrome(self, x: str) -> bool:
return x == x[::-1]"
1;Задача;Программа использует вложенные циклы, которые могут быть недостаточно удобочитаемыми. Как вы будете его оптимизировать?;"1. Разделите логику на отдельные функции: Если вложенные циклы выполняют определенные задачи или операции, рассмотрите возможность выделения этой логики в отдельные функции. Это позволит разделить код на более маленькие и понятные блоки, которые легче понимать и тестировать.
nested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
flattened_list = [item for sublist in nested_list for item in sublist]
2. Используйте генераторы списков (list comprehension): они могут заменить многие случаи вложенных циклов и сделать код более читабельным. 
3. Избегайте избыточных итераций: В некоторых случаях можно избежать полного перебора всех элементов, если известны некоторые условия или предположения о данных. Например, можно использовать операторы break или continue для прекращения итерации или перехода к следующей итерации внутреннего цикла, когда достигнуто определенное условие."
1;Задача;Написать функцию, которая принимает список и возвращает этот же список оставив в нём только уникальные значения не меняя последовательности появления этих значений в изначальном списке. Сложность решения. Оптимизация решения.;"def get_unique_elements(lst):
unique_lst = []
for item in lst:
if item not in unique_lst:
unique_lst.append(item)
return unique_lst
Сложность данной реализации составляет O(n^2), где n - количество элементов в списке. Это происходит из-за вложенного цикла, в котором мы проверяем каждый элемент списка с каждым элементом уже уникального списка.
Оптимизация - Использование OrderedDict из модуля collections для сохранения порядка элементов, но с учетом уникальности. Это позволяет проверять уникальность элементов за линейное время (O(n)) и сохранять порядок элементов в результате.

from collections import OrderedDict

def get_unique_elements(lst):
unique_dict = OrderedDict.fromkeys(lst)
return list(unique_dict.keys())"
1;Задача;Как убрать из списка повторяющиеся элементы;"С использованием множества (set):
my_list = [1, 2, 3, 3, 4, 4, 5]
unique_list = list(set(my_list))

В этом примере мы создаем множество из списка, что автоматически удаляет повторяющиеся элементы, а затем преобразуем его обратно в список.

С использованием генератора списка:
my_list = [1, 2, 3, 3, 4, 4, 5]
unique_list = [x for x in my_list if my_list.count(x) == 1]

В этом примере мы создаем новый список, включая только те элементы, которые встречаются только один раз в исходном списке."
1;Задача;"Вывести словари, где значение ключа b > 150
a = [{‘a’: 140, ‘b’: 100}, {‘a’: 10, ‘b’: 150}, {‘a’: 15, ‘b’: 200}]";"a = [{&apos;a&apos;: 140, &apos;b&apos;: 100}, {&apos;a&apos;: 10, &apos;b&apos;: 150}, {&apos;a&apos;: 15, &apos;b&apos;: 200}]

result = [d for d in a if d[&apos;b&apos;] > 150]

print(result)"
1;Задача;Реализовать генератор при помощи функции;"def filter_dicts(dicts):
for d in dicts:
if d[&apos;b&apos;] > 150:
yield d

a = [{&apos;a&apos;: 140, &apos;b&apos;: 100}, {&apos;a&apos;: 10, &apos;b&apos;: 150}, {&apos;a&apos;: 15, &apos;b&apos;: 200}]

result = filter_dicts(a)

for d in result:
print(d)
В этом примере функция filter_dicts является генератором. Она проходит по каждому словарю d в списке dicts и с помощью ключевого слова yield возвращает словари, у которых значение ключа &apos;b&apos; больше 150. При каждой итерации генератор приостанавливается и сохраняет свое состояние, а затем продолжает выполнение с новыми значениями при следующей итерации."
1;Задача;Есть два списка, вывести только те элементы, что встречаются только в первом списке;"list1 = [1, 2, 3, 4, 5]
list2 = [4, 5, 6, 7, 8]

result = list(set(list1) - set(list2))

print(result)"
1;Задача;объединить два словаря;"Метод update():
dict1 = {&apos;a&apos;: 1, &apos;b&apos;: 2}
dict2 = {&apos;c&apos;: 3, &apos;d&apos;: 4}

dict1.update(dict2)

print(dict1)

Оператор ** (double asterisk):
dict1 = {&apos;a&apos;: 1, &apos;b&apos;: 2}
dict2 = {&apos;c&apos;: 3, &apos;d&apos;: 4}

merged_dict = {**dict1, **dict2}

print(merged_dict)"
1;Задача;"Написать функцию принимает массив, например 1,2,2,3,4,4,3,3
Вернуть массив с кортежами первым элементов число, вторым сколько оно раз повторяется:
[(1,1), (2,2), (3,1)….]";"def count_elements(nums):
count_dict = {}
for num in nums:
if not num in count_dict:
count_dict[num] = 0
count_dict[num]+=1
return list(count_dict.items())

print(count_elements([1,2,2,3,4,4,3,3]))"
1;Deep inside;"Пожалуйста, объясните следующие результаты кода, выполняемого в интерпретаторе оболочки Python:
a=256
b=256
a is b # True
x=257
y=257
x is y # False";"В Python определенный диапазон целых чисел (-5 до 256) интернирован, что означает, что для этих чисел создается и используется один и тот же объект в памяти. Поэтому при сравнении объектов с использованием оператора is, объекты с одинаковыми значениями из этого диапазона будут считаться одним и тем же объектом, и результат будет True.
В этом случае переменные x и y также инициализируются с одинаковым значением 257. Однако число 257 находится за пределами интернированного диапазона. Поэтому Python создает два отдельных объекта в памяти для переменных x и y. В результате оператор is возвращает False, так как x и y ссылаются на разные объекты."
1;Задача;"Каким будет результат следующего выражения?
>>> [0, 1][10:]";"Выражение [0, 1][10:] обращается к срезу списка [0, 1] с позиции 10 и далее.

В данном случае, исходный список [0, 1] содержит только два элемента на позициях 0 и 1. Когда мы запрашиваем срез, начиная с позиции 10 и далее, мы находимся за пределами допустимого диапазона индексов этого списка.

Результатом будет пустой список, так как запрашиваемый срез находится за пределами длины исходного списка.

Поэтому результатом выражения [0, 1][10:] будет пустой список []."
1;Задача;"Каким будет результат следующего выражения?
>>> len(&apos; &apos;.join(list(map(str, [[0], [1]]))))";"Результатом выражения len(&apos; &apos;.join(list(map(str, [[0], [1]])))) будет целое число, равное 7.

Давайте разберемся по шагам:

map(str, [[0], [1]]) преобразует список списков [[0], [1]] в список строк [&apos;[0]&apos;, &apos;[1]&apos;], приводя каждый элемент к строковому типу.

list(map(str, [[0], [1]])) создает новый список [&apos;[0]&apos;, &apos;[1]&apos;], содержащий преобразованные строки.

&apos; &apos;.join(list(map(str, [[0], [1]]))) объединяет элементы списка в одну строку, вставляя между ними пробелы. Результатом будет строка &apos;[0] [1]&apos;.

len(&apos; &apos;.join(list(map(str, [[0], [1]])))) вычисляет длину полученной строки. В данном случае длина строки составляет 7 символов (символы [, 0, ], пробел, [, 1, ]).

Таким образом, результат выражения len(&apos; &apos;.join(list(map(str, [[0], [1]])))) будет равен 7."
1;Задача;"Что будет напечатано в результате выполнения следующего кода? 
arr = [[]] * 5
arr_1, arr_2 = arr, arr
for k, arr in enumerate((arr_1, arr_2)):
arr[0].append(k)
arr = (arr_1, 5, arr_2)
print(arr)";"Вывод в консоли: ([0, 1], 5, [0, 1]).

Первоначально arr представляет собой список из одного пустого списка, который умножается на 5, в результате чего arr представляет собой список из 5 ссылок на один и тот же внутренний пустой список. Затем arr_1 и arr_2 устанавливаются в этот же список. Функция enumerate() вызывается для кортежа, содержащего arr_1 и arr_2, который перебирает обе переменные одновременно с переменной цикла k. Для каждой итерации цикла arr присваивается текущей переменной в кортеже, это означает, что на первой итерации arr присваивается arr_1, а на второй итерации arr присваивается arr_2. Текущий внутренний список, присвоенный arr, затем модифицируется путем добавления значения переменной цикла k к его первому элементу. Наконец, arr переназначается кортежу, содержащему arr_1, целое число 5 и arr_2. Когда этот кортеж печатается, он показывает модифицированный внутренний список, на который ссылаются как arr_1, так и arr_2, целое число 5 и снова модифицированный внутренний список, на который ссылаются как arr_1, так и arr_2."
1;Задача;"Что будет напечатано в результате выполнения следующего кода? 
import sys
arr_1 = []
arr_2 = arr_1
print(sys.getrefcount(arr_1))";"Результатом выполнения кода будет число, которое показывает количество ссылок на объект arr_1.

В данном случае, объект arr_1 является пустым списком [].

Переменная arr_2 присваивается ссылку на arr_1, что означает, что обе переменные arr_1 и arr_2 ссылаются на один и тот же объект в памяти.

Функция sys.getrefcount() возвращает количество ссылок на объект в памяти. Она автоматически учитывает свою собственную ссылку, поэтому результат, который будет напечатан, будет на 1 больше, чем количество явных ссылок на объект.

Таким образом, если выполнить код print(sys.getrefcount(arr_1)), то будет напечатано число 3. Это означает, что к моменту вызова функции sys.getrefcount(), у нас имеется три ссылки на объект arr_1: сама переменная arr_1, переменная arr_2, а также временная ссылка, созданная функцией sys.getrefcount() для подсчета ссылок."
1;Задача;Вам нужно реализовать функцию, которая должна использовать статическую переменную. Вы не можете писать код вне функции и у вас нет информации о внешних переменных (вне вашей функции). Как это сделать?;"def my_function():
static_var = 0
def inner_function():
nonlocal static_var
static_var += 1
return static_var
return inner_function

# создаем объект функции, который использует статическую переменную
f = my_function()

# вызываем функцию несколько раз, чтобы увидеть изменение значения статической переменной
print(f()) # выводит 1
print(f()) # выводит 2
print(f()) # выводит 3"
1;Задача;Напишите однострочник, который будет подсчитывать количество заглавных букв в файле.;"count = sum(1 for line in open(&apos;file.txt&apos;) for char in line if char.isupper())"
1;Задача;Приведите пример использования filter и reduce над итерируемым объектом.;"Пример использования filter() и reduce() над итерируемым объектом в Python:

from functools import reduce

numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# Пример использования filter() для отфильтровывания четных чисел
even_numbers = list(filter(lambda x: x % 2 == 0, numbers))
print(even_numbers) # выводит [2, 4, 6, 8, 10]

# Пример использования reduce() для нахождения суммы чисел от 1 до 10
sum_of_numbers = reduce(lambda x, y: x + y, numbers)
print(sum_of_numbers) # выводит 55
В этом примере мы использовали filter() для отбора только четных чисел в списке numbers, и reduce() для нахождения суммы всех чисел в списке от 1 до 10.

filter() принимает два аргумента - функцию-предикат и итерируемый объект. Он возвращает новый итератор, содержащий только те элементы итерируемого объекта, которые удовлетворяют условиям, заданным функцией-предикатом.

reduce() также принимает два аргумента - функцию и итерируемый объект. Он выполняет функцию на каждой паре элементов из итерируемого объекта, образуя редуцированное значение, которое в конечном итоге становится результатом функции. В примере мы использовали reduce() для нахождения суммы всех чисел в итерируемом объекте."
1;Задача; Как перевести строку, содержащую двоичный код (1 и 0), в число?;"Имеется в виду строка, содержащая двоичное число в виде единиц и нулей.
Ну что же, возьмём int с двумя параметрами - первый строка, а второй - основание системы счисления.

int(""1110101"", 2)
>>>117

Попробуем 16-ричную систему счисления.

int(""0x300CAD"", 16)
>>>3148973

Дадим интерпретатору ""угадать"" систему счисления по префиксу ""0x"", указав вторым параметром 0.

int(""0xdeadbeef"", 0)
>>>3735928559

Префикс ""0b"" указывает, что строка именно двоичная, и ответ будет не 111, а 7.

int(""0b111"", 0)
>>>7"
1;Инструменты;Как сделать python-скрипт исполняемым в различных операционных системах?;"Для того чтобы сделать Python-скрипт исполняемым в различных операционных системах, можно воспользоваться утилитой PyInstaller, которая позволяет упаковать скрипт в исполняемый файл для Windows, Linux и macOS.

Чтобы установить PyInstaller, можно выполнить следующую команду в командной строке:

pip install pyinstaller
После установки PyInstaller необходимо перейти в директорию с Python-скриптом и запустить утилиту с соответствующими параметрами для создания исполняемого файла. Например:

pyinstaller myscript.py --onefile
Эта команда создаст единый исполняемый файл myscript.exe (для Windows) или myscript (для Linux/macOS), который можно запустить на соответствующих операционных системах.

Если нужно создать исполняемый файл с определенными параметрами, можно воспользоваться другими параметрами PyInstaller, такими как --icon для добавления иконки, --name для задания имени исполняемого файла и т.д.

Но стоит отметить, что PyInstaller не является универсальным решением и возможна потребность в использовании других инструментов в зависимости от конкретной задачи и требований к исполняемому файлу."
1;Инструменты;Зачем нужен pdb?;"pdb - это интерактивный отладчик для Python, с помощью которого можно перемещаться по коду во время запуска вашей программы, смотреть и изменять значения переменных, построчно навигироваться по коду (в том числе углубляться во вложенности кода), назначать брейкпоинты и все прочие операции присущие отладчику.

Модуль pdb предоставляет интерфейс командной строки, который можно использовать для взаимодействия с кодом Python во время его выполнения. Вы можете войти в режим pdb в своей программе Python, вставив следующую строку кода там, где вы хотите остановить отладчик: импортировать PDB;

import pdb; 
pdb.set_trace()
Когда интерпретатор дойдет до этой строки, он приостановится, и можно использовать команды pdb для проверки состояния вашей программы. Таким образом, pdb — это полезный инструмент для отладки кода Python, поскольку он позволяет в интерактивном режиме проверять состояние кода и выявлять проблемы."
1;Инструменты;Как перезагрузить импортированный модуль?;"Чтобы перезагрузить импортированный модуль в Python, вы можете использовать функцию reload() из модуля importlib. Вот как это сделать:

from importlib import reload
import module_name

reload(module_name)
Замените module_name на фактическое имя модуля, который вы хотите перезагрузить.

Это может быть полезно при разработке и тестировании модулей, но не рекомендуется использовать в производственном коде без серьезных причин."
1;Инструменты;С помощью каких инструментов можно выполнить статический анализ кода?;"Для статического анализа кода есть несколько инструментов:

Pylint - это инструмент, который анализирует исходный код на соответствие PEP8, а также предупреждает о потенциальных ошибках в коде.

Flake8 - это комбинированный инструмент, который объединяет в себе Pylint, PyFlakes и множество других правил, обеспечивающих соответствие стиля написания кода и обнаруживающих ошибки в исходном коде.

Mypy - это статический типизатор для Python, который позволяет находить ошибки в типах переменных в исходном коде.

Bandit - это инструмент для поиска уязвимостей в исходном коде Python.

Black - это инструмент для автоматического форматирования кода Python, который придерживается только одного стиля написания кода.

Pycodestyle — это простая консольная утилита для анализа кода Python, а именно для проверки кода на соответствие PEP8. Один из старейших анализаторов кода, до 2016 года носил название pep8, но был переименован по просьбе создателя языка Python Гвидо ван Россума.

Vulture — это небольшая утилита для поиска “мертвого” кода в программах Python. Она использует модуль ast стандартной библиотеки и создает абстрактные синтаксические деревья для всех файлов исходного кода в проекте. Далее осуществляется поиск всех объектов, которые были определены, но не используются. Vulture полезно применять для очистки и нахождения ошибок в больших базовых кодах.

Эти инструменты могут улучшить качество кода, облегчить его чтение и поддержку, а также помочь избежать ошибок, связанных с типами переменных и уязвимостями безопасности."
1;Итератор;Что такое итератор?;"Итератор - это объект со магическим методом ""__next__"" для перебора значений по очереди. Когда мы делаем так:

numbers = [1, 2, 3, 4, 5]
for number in numbers:
print(number)

Мы перебираем (итерируем) значения списка. Иногда бывает очень полезно реализовать в коде обработку каких-либо данных именно перебором.
На сайте RealPython есть хороший пример:

class SquareIterator:
def __init__(self, sequence):
self._sequence = sequence
self._index = 0

def __iter__(self):
return self

def __next__(self):
if self._index < len(self._sequence):
square = self._sequence[self._index] ** 2
self._index += 1
return square
else:
raise StopIteration
for square in SquareIterator([1, 2, 3, 4, 5]):
print(square)

>>>1
>>>4
>>>9
>>>16
>>>25

Зачем нужен __iter__, когда есть __next__?
Так же, как у классов есть функция под названием __init__, предназначенная для инициализацию объекта, у итератора есть __iter__, в котором можно выполнить операции инициализации нужных данных и обязательно нужно вернуть объект итератора. В приведенном примере класс в методе __iter__ возвращает сам себя и затем перебирает значения по __next__."
1;Итератор;Чем отличаются __iter__ и __next__?;"iter и next являются методами специальных методов в Python, которые обеспечивают поддержку итерации для объектов.

Метод iter возвращает объект, который может быть использован для итерации по элементам контейнера. Объект, возвращаемый iter, должен содержать метод next.

Метод next должен вернуть следующий элемент в итерации или вызвать исключение StopIteration, если элементов больше нет.

Таким образом, метод iter используется для создания итератора, а метод next используется для перехода к следующему элементу в итерации.

В общем случае, класс должен определять метод iter, который возвращает сам объект класса, и метод next, который определяет, какие элементы будут возвращены при итерации.

Например:

class MyIterator:
def __init__(self, data):
self.index = 0
self.data = data

def __iter__(self):
return self

def __next__(self):
if self.index >= len(self.data):
raise StopIteration
result = self.data[self.index]
self.index += 1
return result
Метод iter возвращает сам объект, а метод next возвращает следующий элемент data каждый раз, когда вызывается."
1;Методы;Для чего нужен метод id()?;"Метод id() используется для интроспекции объектов и возвращает адрес объекта в памяти. Я уже не раз использовал id() в примерах, чтобы определить, являются ли объекты в памяти одним и теми же экземпляром объекта, или нет. Думаю, этот метод все хорошо знают.

В реальной жизни я использую id() при добавлении к классу собственной реализации ""магического метода"""" __repr__ — нечасто, но иногда возникает необходимость как-то иначе выразить строковое представление объекта, и чтобы не путать объект с другими и знать чуть больше, к текстовому описанию я добавляю его id."
1;Методы;Что такое слайс(slice)?;"Слайс (slice) - это способ извлечения определенной части последовательности (например, строки, списка, кортежа) с использованием индексации.

Синтаксис для создания слайса:

sequence[start:end:step]
где start - индекс, с которого начинается извлечение (включительно), end - индекс, на котором заканчивается извлечение (не включая его), и step - шаг для извлечения элементов (по умолчанию равен 1). Обратите внимание, что если не указывать start, то по умолчанию он равен 0, а если не указывать end, то по умолчанию он равен длине последовательности.

Вот пример использования слайса для выбора подряд идущих элементов списка (list):

my_list = [0, 1, 2, 3, 4, 5]
my_slice = my_list[1:4] # выбираем элементы с индексами от 1 до 3 включительно
print(my_slice) # выведет [1, 2, 3]
В этом примере мы использовали слайс my_list[1:4] для выбора элементов списка с индексами от 1 до 3 включительно."
1;Новиночки;Какие новые функции добавлены в python 3.10?;"Python 3.10 включает несколько новых функций и улучшений, в том числе:

Структурное сопоставление с шаблоном: новый синтаксис для сопоставления значений с шаблонами и выполнения различных путей кода на основе совпадения.
Менеджеры контекста в скобках: новый синтаксис, который позволяет использовать произвольные выражения в качестве менеджеров контекста в операторах with.
Улучшенные сообщения об ошибках: Python 3.10 включает множество улучшений сообщений об ошибках, которые отображаются при возникновении ошибок, обеспечивая более полезную и информативную обратную связь.
Новые и улучшенные функции производительности: в Python 3.10 было сделано несколько улучшений производительности, в том числе более быстрое время запуска и уменьшенное использование памяти.
Другие языковые функции. Python 3.10 включает ряд других языковых функций и улучшений, таких как улучшенная поддержка объединений в аннотациях типов, новые параметры форматирования строк и улучшенная поддержка информации о часовых поясах.
Это лишь некоторые из многих новых функций и улучшений в Python 3.10. Для большего информации, вы можете ознакомиться с официальной документацией Python или различными онлайн-ресурсами, которые более подробно освещают новые изменения."
1;ООП;В классах Python, в чем разница между методами класса и статическими методами?;"Метод класса может работать с атрибутами класса, но не с атрибутами экземпляров класса, так как у него нет ссылки на экземпляр, только на сам класс. 
Статический метод не имеет доступа ни к атрибутам класса, ни к атрибутам экземпляров класса. Это независимая самостоятельная функция, объявленная внутри класса, связана с тематикой класса."
1;ООП;Поддерживает ли Python также абстрактные классы?;"Да, Python поддерживает абстрактные классы через модуль abc (Abstract Base Classes). Абстрактные классы позволяют определить интерфейс и некоторое общее поведение для классов-наследников, но сами по себе не могут быть инстанциированы.

Вот пример использования абстрактных классов в Python:
from abc import ABC, abstractmethod

# Абстрактный класс
class Shape(ABC):
@abstractmethod
def area(self):
pass

@abstractmethod
def perimeter(self):
pass

# Наследник абстрактного класса
class Rectangle(Shape):
def __init__(self, width, height):
self.width = width
self.height = height

def area(self):
return self.width * self.height

def perimeter(self):
return 2 * (self.width + self.height)

# Создание экземпляра наследника
rect = Rectangle(5, 3)
print(rect.area()) # Вывод: 15
print(rect.perimeter()) # Вывод: 16
В этом примере абстрактный класс Shape определяет интерфейс, состоящий из двух абстрактных методов area() и perimeter(). Класс Rectangle является наследником Shape и обязан реализовать эти два метода.

При создании экземпляра класса Rectangle мы можем вызывать методы area() и perimeter(), которые определены в классе Rectangle, но мы не можем создавать экземпляры самого абстрактного класса Shape.

Абстрактные классы в Python предоставляют средство для определения общего интерфейса и поведения для группы классов, способствуя структуре кода и обеспечивая согласованность между классами-наследниками."
1;ООП;Для чего используется функция __init__?;Инициализатор объекта класса. Вызывается сразу после создания экземпляра класса и устанавливает атрибуты в нем.
1;ООП;Что делает new;"Вызывается для создания объекта класса в памяти и возвращает адрес нового объекта. После успешного создания объекта вызывается __init__
class SomeClass:
def __new__(cls, *args, **kwargs):
instance = super().__new__(cls)
# В этом месте можно настроить свой экземпляр...
return instance
def __init__(self, val):
self.val = val"
1;ООП;ООП: принципы и зачем оно нужно;"Давайте вспомним основные принципы ООП. Наследование, инкапсуляция и полиморфизм.

С наследованием, думаю, все понятно. Наверняка каждый создавал класс наследованный от чего-либо, начиная с dict и далее.

class NoneDict(dict):
def missing(self, key):
return None

nd = NoneDict({&apos;a&apos;: 1, &apos;b&apos;:2})
print(nd[&apos;a&apos;], nd[&apos;b&apos;], nd[&apos;c&apos;])

>>>1 2 None

Рассматриваем дальше. Инкапсуляция.
""Под инкапсуляцией в объектно-ориентированном программировании понимается упаковка данных и методов для их обработки вместе, т. е. в классе."" Очевидно, что это реализовано тоже, и каждый экземпляр класса всё своё носит с собой.

""Полиморфизм позволяет обращаться с объектами разных классов так, как будто они являются объектами одного класса.""
Тоже весьма очевидно. Наследование и перегрузка методов (плюс магические методы) позволяют нам при желании взаимодействовать с классами универсально и единообразно.

На самом деле, мы используем эти принципы ежедневно, даже не задумываясь о том, что ""мы используем здесь такой-то принцип, потому что..."". Очевидно, что язык явным и естественным образом направляет нас к единственно правильному способу сделать ""это"" (см. PEP20)."
1;ООП;Что такое MRO? Как это работает?;"MRO (Method Resolution Order) - определяет, в каком порядке Python ищет методы при вызове в экземпляре класса или при наследовании.

class A:
def say_hello(self):
print(""Hello from A"")

class B(A):
def say_hello(self):
print(""Hello from B"")

class C(A):
def say_hello(self):
print(""Hello from C"")

class D(B, C):
pass

obj = D()
obj.say_hello()

В данном примере у класса D есть множественное наследование от классов B и C, которые, в свою очередь, наследуют от класса A. В результате, при вызове метода say_hello() у экземпляра класса D, MRO определит, что метод будет искаться сначала в классе D, затем в классе B, далее в классе C, и, наконец, в классе A. В этом случае, будет выведено ""Hello from B"".

Проблема алмаза (diamond problem) возникает при множественном наследовании, когда класс A является родительским для классов B и C, а класс D наследует и от B, и от C. Таким образом, возникает неоднозначность, когда класс D пытается наследовать один и тот же метод от обоих классов B и C, которые, в свою очередь, унаследованы от A."
1;ООП;Для чего в классе используется атрибут __slots__?;"В классе Python атрибут __slots__ используется для оптимизации использования памяти и ускорения работы с объектами класса.

Когда вы создаете экземпляр класса, Python обычно создает словарь, где хранятся все атрибуты объекта. Это позволяет добавлять и изменять атрибуты динамически во время выполнения программы. Однако, для классов с большим количеством экземпляров или при работе с большими объемами данных, этот дополнительный словарь может занимать значительное количество памяти и замедлять работу программы.

Атрибут __slots__ позволяет определить список имен атрибутов, которые могут быть у объекта. Когда __slots__ используется, Python создает фиксированную структуру для объекта, которая занимает меньше памяти (отсутствует dict в экземплярах класса) и ускоряет доступ к атрибутам."
1;ООП;Как создать класс без слова class?;"В Python классы создаются с использованием ключевого слова class. Однако, в Python есть возможность динамически создавать классы с помощью функции type.

Функция type может принимать три аргумента: имя класса, базовые классы (в виде кортежа) и словарь, содержащий атрибуты и методы класса. Вот пример создания класса без использования ключевого слова class:
MyClass = type(&apos;MyClass&apos;, (), {&apos;color&apos;: &apos;red&apos;, &apos;method&apos;: lambda self: &apos;Hello&apos;})

obj = MyClass()
print(obj.color) # red
print(obj.method()) #Hello"
1;ООП;Зачем в python используется ключевое слово self?;"Когда мы создаём новый экземпляр класса, мы по сути создаём новый self, то есть новый объект.
""Под капотом"" мы при этом не создаём копию всего кода класса целиком где-то там в памяти. У всех экземпляров одного класса он один на всех, иначе нам бы не хватило никакой памяти, да это и не нужно. Когда мы обращаемся к конкретному экземпляру, Python использует в качестве self конкретный экземпляр с именно его конкретными атрибутами.

Проверим при помощи функции id(), что у нас в двух экземплярах одного и того же класса одинаковое, а что отличается.
class SomeClass():
def __init__(self):
self.attr = 0

def describe(self):
print(f&apos;{id(self)=}, {id(self.attr)=}, {id(self.describe)=}&apos;)



a = SomeClass()
b = SomeClass()

a.describe()
b.describe()

>>> id(self)=140381617994768, id(self.attr)=9793024, id(self.describe)=140381619250816
>>> id(self)=140381618062816, id(self.attr)=9793024, id(self.describe)=140381619250816
# ^-экземпляры разные атрибут пока одинаков метод тоже одинаковый

Помните ""квантовую физику"" с mutable и immutable? Атрибут ""attr"" у нас сейчас один и тот же, но ведь экземпляры-то разные!?
А ну-ка, изменим у экземпляра под именем a его значение, и поглядим, что получится.

a.attr = 1
a.describe()
b.describe()

>>> id(self)=140381617994768, id(self.attr)=9793056, id(self.describe)=140381619250816
>>> id(self)=140381618062816, id(self.attr)=9793024, id(self.describe)=140381619250816

У &apos;a&apos; теперь атрибут attr является другим объектом, это <class &apos;int&apos;> по адресу 9793056.
То есть, Python не копирует всё подряд при создании нового экземпляра. Он делает это только при необходимости.
Не стоило бы мне говорить об id() как об адресе в памяти, лучше думать что это уникальный идентификатор.
Хотите удивиться ещё больше? Меняем 
self.attr = 0
в нашем классе на 
self.attr = 65535
, запускаем заново...

>>> id(self)=140422513277632, id(self.attr)=9793056, id(self.describe)=140422514534016
>>> id(self)=140422513276000, id(self.attr)=140422515259344, id(self.describe)=140422514534016

🧐 А это как это так, self.attr теперь, судя по адресам, у двух экземпляров оказались раскиданы практически по разным вселенным?

А дело в том, что:

Actually, in order to save time and memory costs, Python always pre-loads all the small integers in the range of [-5, 256]. When a new integer variable in this range is declared, Python just references the cached integer to it and won’t create any new object.

Все целые числа от -5 до +256 уже заранее прокэшированы (по сути являются синглтонами, об этом в другой раз) и если ваша переменная попадает в этот диапазон, то Python даже ничего нового не создаёт. Образно говоря, он ""перевешивает"" брелок с надписью ""экземпляра-такого-то-attr"" на ""гвозик"" с подписью ""int=0"".

Подробнее об этом здесь: https://medium.com/techtofreedom/3-facts-of-the-integer-caching-in-python-20ce587f09bb"
1;ООП;Что такое @classmethod, @staticmethod, @property?;"@classmethod - метод класса, может работать с атрибутами класса, но не атрибутами экземпляра, так как не имеет ссылки на экземпляр, только на класс.
@staticmethod - методы, которые не имеют доступ ни к атрибутам класса, ни к атрибутам экземпляров. Независимая самостоятельная функция, объявленная внутри класса и функционально связанная с тематикой класса. Так например, статический метод может использоваться для валидации значений атрибутов.
@property - декоратор property позволяет использовать методы в качестве свойств объектов. Удобный способ работы с __приватными атрибутами. Работать с приватными атрибутами надо через сеттеры и геттеры, чтобы не нарушать внутреннюю логику работы класса."
1;Операторы;"Каким будет результат следующего выражения?

>>> -30 % 10";"Символ % в Python является оператором модуля, то есть возвращает остаток после деления, например: 10 % 3 = 1

Учитываем, что оператор модуля возвращает остаток, имеющий тот же знак, что и делитель, а не делимое, поэтому минус нас смущать не должен. 30 на 10 делится без остатка, получаем ответ 0.

Какой-то короткий вопрос, и даже нечего добавить. Как говорит Артемий Лебедев, ""ну, модуль и модуль."""
1;Определение;Что такое файлы .pth?;"Файлы с расширением .pth - это файлы, которые могут быть использованы для добавления директорий в путь поиска модулей Python. Директивы .pth выполняются при запуске интерпретатора Python и добавляют определенные каталоги в переменную sys.path. Это удобно, когда нужно импортировать модули из нестандартных директорий без необходимости переноса файлов в директории по умолчанию. Использование директив .pth достаточно распространено в мире Python и они встречаются в различных средах разработки и фреймворках, таких как PyTorch.

Файлы .pth могут быть также использованы злоумышленниками для внедрения вредоносного кода в систему Python, так как они могут изменять список каталогов, в которых выполняется поиск модулей Python. Поэтому необходимо быть внимательными при работе с такими файлами и использовать только те файлы .pth, которые вы знаете и доверяете."
1;;Проблема ascii в python 2;
1;Определение;Что такое Cython? Что такое IronPython? Что такое PyPy? Почему они до сих пор существуют и зачем?;"Cython - это язык программирования, нацеленный на увеличение производительности Python-кода. Cython позволяет использовать возможности языка Python и C/C++ для эффективного написания расширений модулей на языке Python. Он позволяет вам писать код на Python, который доступен из C/C++, и наоборот. Cython обеспечивает скорость выполнения, сравнимую со скоростью выполнения на языке C/C++, при этом сохраняя простоту и удобство использования языка Python. Cython compiler компилирует исходный код в C/C++ и затем переводит его в машинный код, что дает быстрый доступ к низкоуровневым ресурсам операционной системы, таким как память и ввод-вывод. Cython также предоставляет возможность использовать дополнительные функции, такие как статическая типизация и параллельное программирование, для дополнительного увеличения производительности.

IronPython - это реализация языка программирования Python, которая работает в контексте платформы .NET. IronPython предоставляет возможность использовать Python в качестве языка .NET. Он может использоваться для написания .NET-приложений, а также для расширения приложений, написанных на других языках .NET. IronPython является открытым и свободно распространяемым программным обеспечением.

PyPy — это высокопроизводительная реализация языка программирования Python. Он был создан с целью предоставления более быстрой и эффективной альтернативы стандартному интерпретатору CPython. PyPy включает компилятор Just-In-Time (JIT), который может оптимизировать выполнение кода Python во время выполнения, что может привести к значительному повышению производительности по сравнению с CPython, особенно для определенных типов рабочих нагрузок. PyPy также поддерживает многие из тех же функций и модулей, что и CPython, включая объектно-ориентированное программирование, динамическую типизацию и стандартную библиотеку Python."
1;Паттерны;SOLID;"1. Принцип единственной ответственности (Single Responsibility Principle, SRP):
Принцип SRP гласит, что класс должен быть ответственен только за одну задачу. Это помогает упростить код и сделать его более поддерживаемым.

2. Принцип открытости/закрытости (Open/Closed Principle, OCP):
Принцип OCP утверждает, что классы и модули должны быть открытыми для расширения, но закрытыми для изменения. То есть стараемся не вносить измнеения в базовый класс, а наследоваться от него и потом расширять функционал. Так ничего из старого кода не сломается.

class Shape(ABC):
@abstractmethod
def calculate_area(self):
pass

class Rectangle(Shape):
def __init__(self, width, height):
self.width = width
self.height = height

def calculate_area(self):
return self.width * self.height

class Circle(Shape):
def __init__(self, radius):
self.radius = radius

def calculate_area(self):
return 3.14 * self.radius ** 2

3. Принцип подстановки Барбары Лисков (Liskov Substitution Principle, LSP):
Принцип LSP гласит, что подклассы должны быть заменяемыми на свои базовые классы без нарушения корректности программы. Это означает, что поведение подкласса не должно противоречить ожиданиям, установленным базовым классом.

class Bird(ABC):
@abstractmethod
def fly(self):
pass

class Duck(Bird):
def fly(self):
print(""Duck is flying"")

class Penguin(Bird):
def fly(self):
raise NotImplementedError(""Penguins can&apos;t fly"")

4. Принцип разделения интерфейса (Interface Segregation Principle, ISP):
Принцип ISP утверждает, что клиенты не должны зависеть от интерфейсов, которые они не используют. Интерфейсы должны быть маленькими и специфичными для потребностей клиентов.
class Printer(ABC):
@abstractmethod
def print_document(self, document):
pass

class Scanner(ABC):
@abstractmethod
def scan_document(self):
pass

class FaxMachine(Printer, Scanner):
def print_document(self, document):
print(""Printing document"")

def scan_document(self):
print(""Scanning document"")

5. Принцип инверсии зависимостей (Dependency Inversion Principle, DIP):
Принцип DIP предлагает зависеть от абстракций, а не от конкретных реализаций. Это достигается через внедрение зависимостей и использование интерфейсов.
class DatabaseConnector(ABC):
@abstractmethod
def connect(self):
pass

class MySQLConnector(DatabaseConnector):
def connect(self):
print(""Connecting to MySQL database"")

class PostgreSQLConnector(DatabaseConnector):
def connect(self):
print(""Connecting to PostgreSQL database"")"
1;Паттерны;KISS;KISS (Keep It Simple, Stupid) - это принцип разработки программного обеспечения, который подразумевает создание простого и понятного кода, избегая излишней сложности и излишней умственной нагрузки.
1;Паттерны;DRY;"DRY (Don&apos;t Repeat Yourself) - это принцип разработки программного обеспечения, который подразумевает избегание повторения одинакового кода или информации в разных частях программы."
1;Паттерны;Почему singleton антипаттерн;"Синглтон (Singleton) - это шаблон проектирования, который позволяет создать только один экземпляр класса. Вот несколько причин, почему синглтон считается антипаттерном:
1. Синглтон нарушает SRP (Single Responsibility Principle) — класс синглтона, помимо того чтобы выполнять свои непосредственные обязанности, занимается еще и контролированием количества своих экземпляров."
1;Типы и структуры данных;Объясните разницу между списками и кортежами в Python. В каких случаях лучше использовать каждый из них?;"Списки и кортежи в Python являются структурами данных для хранения коллекций элементов. Однако существуют некоторые различия между ними:
Изменяемость: Списки являются изменяемыми, что означает, что вы можете изменять их содержимое (добавлять, удалять или изменять элементы). Кортежи, с другой стороны, являются неизменяемыми, то есть их содержимое не может быть изменено после создания.

Пример:
my_list = [1, 2, 3]
my_tuple = (1, 2, 3)
my_list[0] = 100 # Разрешено
my_tuple[0] = 100 # Вызовет ошибку TypeError

Быстродействие: Кортежи обычно работают быстрее, чем списки при выполнении некоторых операций, таких как итерация и доступ к элементам. Это связано с тем, что кортежи имеют структуру данных, которая оптимизирована для чтения, а не записи.
Размер: Кортежи обычно занимают меньше памяти, чем списки, так как они имеют более компактное представление данных. Это делает их более эффективными с точки зрения использования памяти, особенно при работе с большими наборами данных.
Использование: Кортежи обычно используются для представления наборов разнородных данных, которые не изменяются во время выполнения программы, например, координаты точки в пространстве. Списки обычно используются для представления изменяемых наборов однородных данных, например, числовых последовательностей или строк."
1;Типы и структуры данных;Как устроены хэш таблицы?;"Хэш-таблица это разреженный массив (массив, в котором имеются незаполненные позиции). В стандартных англоязычных учебниках ячейки хэш-таблицы называются ""bucket"". В хэш-таблице dict каждому элементу соотвествует ячейка, содержащая два поля: ссылку на ключ и ссылку на значение элемента. Поскольку размер всех ячеек одинаков, доступ к отдельной ячейке производится по смещению.

Python стремится оставить не менее трети ячеек пустыми; если хэш-таблица становится чрезмерно заполненной, то она копируется в новый участок памяти, где есть место для большего числа ячеек.

Для помещения элемента в хэш-таблицу нужно первым делом вычислить хэш-значение ключа элемента. Это делает встроенная функция hash().

Для выборки значения с помощью выражения my_dict[search_key] Python обращается к функции hash(search_key), чтобы получить хэш-значение search_key, и использует несколько младших битов полученного числа как смещение ячейки относительно начала хэш-таблицы (сколько именно битов зависит от текущего размера таблицы). Если найденная ячейка пуста, возбуждается исключение KeyError. В противном случае в найденной ячейке есть какой-то элемент - пара ключ:значение - и тогда Python проверяет, верно ли то, что search_key == found_key. Если да, то элемент найден и возвращается found_value. Если же search_key и found_key не совпали, то имеет место коллизия хэширования. Для разрешения коллизии алгоритм берет различные биты хэш-значения, производит над ними определенные действия и использует результат как смещение другой ячейки."
1;Типы и структуры данных;list, set, dict какая сложность операций вставки, чтения, удаления для каждого.;"Список (list)
Список является одной из самых важных структур данных в Python. Можно использовать списки для создания стека или очереди. Списки — это упорядоченные и изменяемые коллекции, которые можно обновлять по желанию.

Вставка: Начало О(n), Середина О(n), Конец О(1)
Удаление: Начало О(n), Середина О(n), Конец О(1)
Получение элемента: O(1).
Проход: O(n).
Получение длины: O(1).

Множество (set)
Множества также являются одними из наиболее используемых типов данных в Python. Множество представляет собой неупорядоченную коллекцию. Множество не допускает дублирования, и, следовательно, каждый элемент в множестве уникален. Множество поддерживает множество математических операций, таких как объединение, разность, пересечение и так далее.

Проверить наличие элемента в множестве: O(1).
Отличие множества A от B: O(длина A).
Пересечение множеств A и B: O(минимальная длина A или B).
Объединение множеств A и B: O(N) , где N это длина (A) + длина (B).

Словарь (dict)
Словарь — это коллекция пар ключ-значение. Ключи в словаре уникальны, чтобы предотвратить коллизию элементов. Это чрезвычайно полезная структура данных.
Операции со словарями и их временная сложность
Здесь мы считаем, что ключ используется для получения, установки или удаления элемента.

Получение элемента: O(1).
Установка элемента: O(1).
Удаление элемента: O(1).
Проход по словарю: O(n)."
1;Типы и структуры данных;Как dict и set реализованы внутри? Какова сложность получения элемента? Сколько памяти потребляет каждая структура?;"Dict и Set реализованы в виде хэш-таблицы.

Хэш-таблица - это структура данных, которая использует хэш-функцию для преобразования ключа в индекс в массиве, где хранятся значения. Затем элемент добавляется в массив по соответствующему индексу.

Сложность получения элемента в Dict и Set в наилучшем случае составляет O(1), поскольку элемент может быть получен просто с помощью хэш-функции в качестве индекса массива. Однако в худшем случае, когда возникают хэш-коллизии, сложность может вырасти до O(n), где n - количество элементов в таблице.

Также стоит заметить, что сложность операций добавления, удаления и поиска элементов в Set и Dict также составляет O(1) в наилучшем случае и O(n) в худшем случае."
1;Типы и структуры данных;Что может быть ключом в словаре?;"Для начала простыми словами о том, что такое хэш. Это математическая функция, приводящая различные массивы данных в строку фиксированной длины. Одинаковые массивы данных (например, одна и та же строка в разных переменных) будут иметь одинаковый хэш.

А далее просто: любой хэшируемый тип, а значит, immutable - имеет метод hash, который на самом и является ключом словаря. Словарь - это хэш-таблица, то есть ""это структура данных, реализующая интерфейс ассоциативного массива, а именно, она позволяет хранить пары (ключ, значение) и выполнять три операции: операцию добавления новой пары, операцию удаления и операцию поиска пары по ключу.""

Википедия говорит нам ""Важное свойство хеш-таблиц состоит в том, что, при некоторых разумных допущениях, все три операции (добавление, удаление, поиск элемента) в среднем выполняются за время O(1).

То есть, максимально быстро.

Как обычно, мимоходом я затронул другую тему - оценку сложности алгоритмов, которая часто упоминается как Big O Notation. Не буду углубляться, просто скажу, что O(1) означает минимальные затраты времени и количества операций, сложность поиска слабо возрастает с ростом количества элементов структуры данных (нашего словаря.)"
1;Функции;Что такое лямбда-функции в Python? Пример, в котором используется лямбда-функции?;"О лямбда-функциях. По сути, это всего лишь синтаксический сахар, необходимый для создания анонимных функций ради упрощения кода и экономии места. Как правило, lambda-выражения используются при вызове функций (или классов), которые принимают функцию в качестве аргумента.
Встроенная функция сортировки Python принимает функцию в качестве ключевого аргумента. Эта ключевая функция использует для вычисления сравнительного ключа при определении порядка сортировки элементов.

Таким образом, сортировка — отличный пример места, в котором используются лямбда-выражения:
colors = [""Goldenrod"", ""purple"", ""Salmon"", ""turquoise"", ""cyan""]
print(sorted(colors, key=lambda s: s.casefold()))"
1;Функции;Что такое *args и **kwargs в определении функции?;"*args — это сокращение от «arguments» (аргументы), а **kwargs — сокращение от «keyword arguments» (именованные аргументы).
Каждая из этих конструкций используется для распаковки аргументов соответствующего типа, позволяя вызывать функции со списком аргументов переменной длины. "
1;Функции;Как аргументы передаются в функции: по значению или по ссылке?;"Python passes references-to-objects by value (like Java), and everything in Python is an object. This sounds simple, but then you will notice that some data types seem to exhibit pass-by-value characteristics, while others seem to act like pass-by-reference... what&apos;s the deal?

It is important to understand mutable and immutable objects. Some objects, like strings, tuples, and numbers, are immutable. Altering them inside a function/method will create a new instance and the original instance outside the function/method is not changed. Other objects, like lists and dictionaries are mutable, which means you can change the object in-place. Therefore, altering an object inside a function/method will also change the original object outside.

И у нас выходит, что передаются ссылки на объекты по их, ссылок, значению. И если объект immutable, то при его изменении создаётся копия объекта, которая и изменяется внутри функции/метода.

Проверим!

def updateNumber(n):
print(id(n)) # 3151030976880
n += 10
print(id(n)) # 3151030977200 <--- id изменился

b = 5
print(id(b)) # 3151030976880
updateNumber(b) # 3151030976880
print(b) # 5

id - это идентификатор объекта, и он изменился. То есть, теперь у нас есть ""локальная копия"" переданного объекта внутри функции.

Не каждый интервьюер сам ответит на этот вопрос правильно. Это почти ""квантовое"" и совсем неявное копирование ведет к заблуждению, что разные типы объектов где-то там, ""под капотом"", передаются в функцию/метод по-разному.

На самом же деле, передаются они одинаково, но вот объекты ведут себя по-разному."
1;;Что такое контекстные менеджеры и как их создать? В каких случаях контекстные менеджеры могут быть полезны?;"Контекстные менеджеры в Python - это объекты, которые определяют методы для установки и освобождения ресурсов в пределах блока кода. Они часто используются для обеспечения корректного и безопасного использования ресурсов, таких как открытие и закрытие файлов, установка и освобождение соединений с базой данных или сетевых соединений.

Контекстные менеджеры используются вместе с оператором with. Они автоматически выполняют определенные действия при входе и выходе из блока кода, определенного оператором with.

Чтобы создать контекстный менеджер, нужно определить класс с двумя специальными методами:

__enter__(self): Метод, который вызывается при входе в блок оператора with. Он может выполнять действия по инициализации ресурсов, такие как открытие файла или установка соединения. Метод __enter__ может также возвращать объект, который будет использоваться внутри блока with.
__exit__(self, exc_type, exc_value, traceback): Метод, который вызывается при выходе из блока оператора with, независимо от того, возникло ли исключение. Он может выполнять действия по освобождению ресурсов, такие как закрытие файла или соединения. Метод __exit__ принимает три аргумента, которые содержат информацию об исключении, если оно возникло, иначе None.
Пример контекстного менеджера для работы с файлами:

class FileContextManager:
def __init__(self, file_name, mode):
self.file_name = file_name
self.mode = mode
def __enter__(self):
self.file = open(self.file_name, self.mode)
return self.file
def __exit__(self, exc_type, exc_value, traceback):
self.file.close()
# Использование контекстного менеджера
with FileContextManager(""example.txt"", ""r"") as file:
content = file.read()
print(content)
В этом примере мы создаем контекстный менеджер FileContextManager, который открывает файл при входе в блок with и закрывает его при выходе. Это гарантирует корректное закрытие файла даже в случае возникновения исключений внутри блока.

Контекстные менеджеры полезны в следующих случаях:

Управление ресурсами, требующими инициализации и освобождения, такими как файлы, сетевые соединения, соединения с базами данных и т. д.
Управление состоянием или конфигурацией, которая должна быть временно изменена и затем восстановлена, например, изменение текущей директории, уровня логирования или блокировки потоков.
Управление контекстом исполнения, когда нужно выполнять определенные действия перед и после выполнения блока кода, например, измерение времени выполнения или вход и выход из критической секции."
1;;Python язык функционального программирования или объект-ориентированного;Python является мультипарадигмальным языком программирования, что означает, что он поддерживает несколько парадигм, включая объектно-ориентированное программирование (ООП) и функциональное программирование (ФП).
1;;Объясните разницу между функциями map(), filter() и reduce(). Приведите примеры использования каждой из этих функций.;"Функции map(), filter() и reduce() - это встроенные функции высшего порядка в Python, которые используются для обработки списков и других последовательностей. Они принимают функцию и последовательность в качестве аргументов и применяют функцию к элементам последовательности, выполняя различные операции.

map(): Эта функция применяет заданную функцию к каждому элементу последовательности и возвращает новую последовательность с результатами. map() принимает два аргумента: функцию и последовательность (например, список).
Пример использования map():

def square(x):
return x * x
numbers = [1, 2, 3, 4, 5]
squared_numbers = map(square, numbers)
# Преобразуем результат в список
squared_numbers_list = list(squared_numbers)
print(squared_numbers_list) # Вывод: [1, 4, 9, 16, 25]
filter(): Эта функция фильтрует элементы последовательности на основе предиката (функции, которая возвращает True или False). filter() принимает два аргумента: предикат и последовательность. Функция возвращает новую последовательность, состоящую только из элементов, для которых предикат вернул True.
Пример использования filter():

def is_even(x):
return x % 2 == 0
numbers = [1, 2, 3, 4, 5]
even_numbers = filter(is_even, numbers)
# Преобразуем результат в список
even_numbers_list = list(even_numbers)
print(even_numbers_list) # Вывод: [2, 4]
reduce(): Эта функция выполняет свертку последовательности, применяя заданную функцию к элементам последовательности попарно, последовательно уменьшая количество элементов. reduce() принимает два аргумента: функцию и последовательность. Функция должна принимать два аргумента и возвращать одно значение. reduce() возвращает одно значение - результат свертки.
Пример использования reduce():

from functools import reduce
def multiply(x, y):
return x * y
numbers = [1, 2, 3, 4, 5]
product = reduce(multiply, numbers)
print(product) # Вывод: 120 (1 * 2 * 3 * 4 * 5)
В этом примере мы используем функцию reduce() для вычисления произведения всех элементов списка numbers.

Обратите внимание, что функция reduce() должна быть импортирована из модуля functools.

Эти функции можно использовать с анонимными функциями (lambda-функциями) для более краткого и удобного синтаксиса. Примеры использования map(), filter() и reduce() с lambda-функциями:

numbers = [1, 2, 3, 4, 5]
# Использование map() с lambda-функцией для получения квадратов чисел
squared_numbers = map(lambda x: x * x, numbers)
squared_numbers_list = list(squared_numbers)
print(squared_numbers_list) # Вывод: [1, 4, 9, 16, 25]
# Использование filter() с lambda-функцией для фильтрации четных чисел
even_numbers = filter(lambda x: x % 2 == 0, numbers)
even_numbers_list = list(even_numbers)
print(even_numbers_list) # Вывод: [2, 4]
# Использование reduce() с lambda-функцией для вычисления произведения всех чисел
from functools import reduce
product = reduce(lambda x, y: x * y, numbers)
print(product) # Вывод: 120 (1 * 2 * 3 * 4 * 5)
В этом примере мы используем lambda-функции вместо обычных функций для более краткого и удобного синтаксиса при использовании функций map(), filter() и reduce().

Важно отметить, что с использованием генераторов списков и выражений-генераторов можно добиться аналогичной функциональности, часто с более лаконичным и читаемым синтаксисом."
1;;память (очистка и тд);"В Python существует автоматическое управление памятью, основанное на сборке мусора (garbage collection). Сборка мусора отслеживает и освобождает память, занимаемую объектами, которые больше не используются в программе.

Процесс сборки мусора в Python основан на концепции подсчета ссылок. Каждый объект имеет счетчик ссылок, который отслеживает, сколько ссылок ведет к этому объекту. Когда счетчик ссылок объекта становится равным нулю, это означает, что на объект больше нет активных ссылок, и он может быть безопасно освобожден из памяти.

Сборщик мусора Python автоматически определяет, когда объект больше не доступен для программы, и освобождает память, занимаемую этим объектом. Он использует алгоритм под названием ""reference counting"" (подсчет ссылок), чтобы отслеживать счетчики ссылок на объекты.

Кроме того, Python также использует другой механизм сборки мусора, называемый ""циклическим сборщиком мусора"" (cycle collector), для обнаружения и освобождения объектов, на которые есть взаимные ссылки и которые не могут быть достигнуты из корневого набора объектов."
1;;Как получить документацию по атрибутам объекта?;"В Python вы можете получить документацию по атрибутам объекта с помощью атрибута doc. Например, если у вас есть объект с атрибутом attribute_name, то вы можете получить его документацию следующим образом:

print(attribute_name.__doc__)
Вы также можете использовать встроенную функцию help() для получения подробной информации о любом объекте, включая его атрибуты. Просто передайте объект в функцию help(), чтобы получить всю доступную документацию:

help(attribute_name)
Небольшое уточнение: doc отображает документацию для конкретного атрибута или метода. Если вы хотите получить общую документацию для объекта, вызовите help() без параметров (т.е. help(object_name)).

Например, если у вас есть класс с атрибутом attribute_name, вы можете получить его документацию следующим образом:

class MyClass:
""""""This is the docstring for MyClass.""""""
attribute_name = ""value""

print(MyClass.attribute_name.__doc__)
Этот код выведет документацию для атрибута attribute_name, которая будет равна None, так как мы не определили документацию для него в классе. Теперь мы можем использовать функцию help() для получения документации для самого класса:

help(MyClass)
Это приведет к выводу всей доступной документации для MyClass, включая документацию для его атрибута attribute_name."
1;;Как просмотреть методы объекта?;"Интроспекция — это способность объекта во время выполнения получить тип, доступные атрибуты и методы, а также другую информацию, необходимую для выполнения дополнительных операций с объектом.
Python поддерживает полную интроспекцию. Это означает, что для любого объекта можно получить всю информацию о его внутренней структуре. Применение интроспекции является важной частью того, что называют pythonic style, и широко применяется в библиотеках и фреймворках Python.

Инструменты интроспекции - это:
dir(), type(), id(), hasattr(), isinstance().

Используются они, соответственно, для:

dir() - получения списка всех атрибутов и методов объекта
type() - получения типа объекта
id() - идентификатора (адреса в памяти) объекта
hasattr() - проверки на наличие атрибута
isinstance() - проверки на принадлежность к классу или подклассу

добавим в этот же список и callable() - проверка, является ли указанное ""свойство"" объекта вызываемым (callable) методом.

Ответ, казалось бы, простой - использовать dir().
Но вопрос был задан именно о методах, а про атрибуты нам ничего не было сказано. Если это не ошибка задающего вопрос, а намеренное умолчание, то нам придется перебрать все свойства, полученные из dir() и проверить, являются ли они методами.

some = &apos;abc&apos;
print([attr_name for attr_name in dir(some) if callable(getattr(some, attr_name))])

>>>[&apos;__add__&apos;, &apos;__class__&apos;, &apos;__contains__&apos;, &apos;__delattr__&apos;, &apos;__dir__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__getitem__&apos;, &apos;__getnewargs__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__init_subclass__&apos;, &apos;__iter__&apos;, &apos;__le__&apos;, &apos;__len__&apos;, &apos;__lt__&apos;, &apos;__mod__&apos;, &apos;__mul__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__rmod__&apos;, &apos;__rmul__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;capitalize&apos;, &apos;casefold&apos;, &apos;center&apos;, &apos;count&apos;, &apos;encode&apos;, &apos;endswith&apos;, &apos;expandtabs&apos;, &apos;find&apos;, &apos;format&apos;, &apos;format_map&apos;, &apos;index&apos;, &apos;isalnum&apos;, &apos;isalpha&apos;, &apos;isascii&apos;, &apos;isdecimal&apos;, &apos;isdigit&apos;, &apos;isidentifier&apos;, &apos;islower&apos;, &apos;isnumeric&apos;, &apos;isprintable&apos;, &apos;isspace&apos;, &apos;istitle&apos;, &apos;isupper&apos;, &apos;join&apos;, &apos;ljust&apos;, &apos;lower&apos;, &apos;lstrip&apos;, &apos;maketrans&apos;, &apos;partition&apos;, &apos;replace&apos;, &apos;rfind&apos;, &apos;rindex&apos;, &apos;rjust&apos;, &apos;rpartition&apos;, &apos;rsplit&apos;, &apos;rstrip&apos;, &apos;split&apos;, &apos;splitlines&apos;, &apos;startswith&apos;, &apos;strip&apos;, &apos;swapcase&apos;, &apos;title&apos;, &apos;translate&apos;, &apos;upper&apos;, &apos;zfill&apos;]
Восхитимся, как много хорошего и разного можно сотворить с простой строкой! "
1;;Что такое globals() и locals()?;"globals и locals возвращают словари глобальных и локальных переменных. Локальные - это например, те, что находятся в области видимости функции или метода. Глобальные доступны отовсюду.

Но я не могу просто так взять и не упомянуть какую-нибудь особенность.
В globals также присутствуют импортированные модули.

import json
myglobals = globals()
print(f&apos;{myglobals=}\n\r&apos;)
j = myglobals[&apos;json&apos;]
print(j)
print(type(j)) # Всё в Python является объектом, и модуль тоже.

>>> <module &apos;json&apos; from &apos;/usr/lib/python3.8/json/init.py&apos;>
>>> <class &apos;module&apos;> 

Давайте посмотрим в локальные и глобальные переменные.

a = 15

def test():
a = 1 # Эта &apos;a&apos; - в locals
print(a)


test()
print(a) # Эта - глобальная, она и останется ==15

>>> myglobals={&apos;name&apos;: &apos;main&apos;, &apos;doc&apos;: None, &apos;package&apos;: None, &apos;loader&apos;: <_frozen_importlib_external.SourceFileLoader object at 0x7fdb99a471c0>, &apos;spec&apos;: None, &apos;annotations&apos;: {}, &apos;builtins&apos;: <module &apos;builtins&apos; (built-in)>, &apos;file&apos;: &apos;ex15.py&apos;, &apos;cached&apos;: None, &apos;json&apos;: <module &apos;json&apos; from &apos;/usr/lib/python3.8/json/init.py&apos;>, &apos;myglobals&apos;: {...}}


>>> 1
>>> 15

Есть такое правило, по которому резолвятся имена, в английском оно называется LEGB. Local - Enclosing - Global - Builtins. То есть, локальные, нелокальные/вложенные (когда функция внутри другой функции), глобальные и встроенные.

В приведенном примере можно сказать, что глобальная переменная a затеняется локальной переменной a. Это такой термин, ""затенение переменной"". Затеняется она, естественно, внутри функции.

Если мы хотим использовать глобальную переменную в функции, об этом нужно сообщить, дописав &apos;global a&apos; после определения функции - так код будет более понятным. Если же просто убрать строку &apos;a = 1&apos;, функция тоже будет использовать глобальную переменную, т.к. интерпретатор, не имея локальной &apos;a&apos;, пойдет дальше по правилу LEGB и найдёт ту &apos;a&apos;, которую мы и имели в виду. Глобальную в нашем случае."
1;;В чём разница между пакетами и модулями?;"В терминологии.
Модулем мы называем отдельный файл .py, из которого как правило, что-то импортируем, а пакетом - папку с такими модулями и файлом __init__.py, то есть, в пакете может быть несколько модулей. Также при публикации модуля в pypi он станет ""пакетом из одного модуля"", иначе никак. 

Не знаю, чем понятия ""файл с исходником"" и ""папка с исходниками"" хуже, но вот приняли такое наименование."
1;;Как проверить, что один кортеж содержит все элементы другого кортежа?;"Этот вопрос, как и многие другие, не на выдумывание алгоритма перебора, а на знание и умение вовремя применить готовый метод. Не нужно ничего изобретать, не нужно танцевать вокруг элементов кортежа, пытаясь наиболее эффективно перебрать одно через другое.

Самый простой способ - использовать метод issubset:

small = (1,2,3)
big = (4, 5, 6, 7, 1, 2, 4, 4, 3)
set(small).issubset(set(big))

>>>True

Вот вам совет: как только вы замечаете, что заиграла музыка и вас вот-вот унесёт божественный танец - остановитесь и подумайте, вспомните, что ""батарейки в комплекте"". Просто нужно знать, где они лежат, достать и использовать по назначению.

p.s. Туда же:

all(x in big for x in small)

>>>True"
1;;Почему пустой список нельзя использовать как аргумент по умолчанию?;"""Значения по умолчанию для аргументов функции назначаются только один раз, когда функция определена, а не каждый раз при её вызове.""

def add_fruit(fruit, basket=[]):
basket.append(fruit)
return basket

b = add_fruit(""banana"")
print(b)

>>> [&apos;banana&apos;]

c = add_fruit(""apple"")
print(c)

>>> [&apos;banana&apos;, &apos;apple&apos;]

Почему? А всё потому, что:

def add_fruit(fruit, basket=[]):
^^^
Как мы уже рассматривали в вопросах о mutability,
это всегда будет один и тот же список, вместо каждый раз нового пустого и таким образом мы ""выстрелим себе в ногу"".

Соответственно, лучше писать:

def add_fruit(fruit, basket=None):
if basket is None:
basket = []

Но, куда же без нюансов.
Что будет, если мы аннотируем функцию так, как нам хотелось бы?

def add_fruit(fruit:str, basket:list=None):

Наша аннотация начинает выглядеть странно - получается, мы врём сами себе: basket это list, но тут же он None, а так нельзя.

Поэтому напишем вот что:

def add_fruit(fruit: str, basket: list | None = None):

И будет красиво."
1;Определение;Что такое метаклассы в Python, и в каких случаях они могут быть полезны? Объясните принцип работы метаклассов и приведите пример создания и использования метакласса;"Метаклассы в Python - это специальные классы, которые определяют поведение и свойства других классов. Вместо того, чтобы создавать объекты, метаклассы создают классы. В основе работы метаклассов лежит принцип, что классы в Python являются объектами и имеют свой собственный тип, который называется метаклассом.

По сути, метаклассы в Python являются классами, которые создают и управляют другими классами. В Python классы сами являются объектами, и метаклассы определяют поведение этих объектов-классов.

Метаклассы имеют следующие особенности:

Метаклассы являются производными от базового метакласса type. В Python type является метаклассом по умолчанию для всех классов и определяет стандартное поведение создания и управления классами.
Метаклассы позволяют контролировать процесс создания класса, наследования от других классов и определения атрибутов и методов класса. Это делается путем переопределения специальных методов, таких как __new__ и __init__.
Метаклассы могут быть использованы для автоматического добавления атрибутов или методов к классам, регистрации классов, контроля доступа к атрибутам и методам, и т. д. Это делает их мощным инструментом для расширения и модификации поведения классов в Python.
Метаклассы могут быть полезны в следующих случаях:

Валидация атрибутов класса во время определения класса.
Изменение или добавление новых атрибутов или методов класса во время определения класса.
Ограничение способов создания классов или наследования от определенных классов.
Реализация расширенных функций декларативного программирования, таких как описание схем данных или взаимодействие с внешними ресурсами.
Принцип работы метаклассов:

Когда Python создает класс, он вызывает метакласс, который определяет, как будет создан новый класс. Метаклассы обычно наследуются от базового метакласса type и переопределяют методы __new__ и/или __init__ для изменения или добавления поведения при создании класса.

Пример создания и использования метакласса:

class MyMeta(type):
def __new__(mcls, name, bases, cls_dict):
print(f""Создание класса {name}"")
# Модифицируем имя класса, добавляя префикс ""My""
name = ""My"" + name
# Создаем новый класс с измененным именем, используя базовый метакласс
new_class = super().__new__(mcls, name, bases, cls_dict)
return new_class

class MyClass(metaclass=MyMeta):
def __init__(self, x):
self.x = x
def show(self):
print(f""Значение x: {self.x}"")
# Использование класса с метаклассом
obj = MyClass(10)
obj.show()
print(f""Имя класса: {obj.__class__.__name__}"")
В этом примере мы создаем метакласс MyMeta, который изменяет имя создаваемого класса, добавляя префикс ""My"". Затем мы создаем класс MyClass с указанием метакласса MyMeta. При создании объекта MyClass, мы видим, что имя класса было изменено метаклассом на ""MyMyClass"".

Метаклассы предоставляют мощный инструмент для манипулирования классами и их созданием в Python, но часто есть другие подходы, которые могут быть использованы вместо метаклассов. Однако в некоторых случаях метаклассы могут быть наиболее подходящим и элегантным решением. Вот некоторые примеры, где метаклассы могут быть особенно полезны:

Изменение поведения классов на уровне определения класса: Метаклассы позволяют вам модифицировать классы в момент их определения, что может быть полезно для автоматического добавления или изменения атрибутов, методов или свойств класса. В этом случае метаклассы могут быть более удобны, чем другие подходы, такие как использование миксинов или декораторов классов.
Контроль над наследованием и созданием классов: Метаклассы позволяют контролировать процесс создания классов и наследования от других классов. Например, метакласс может предотвратить создание класса, если он не соответствует определенным критериям, или запретить наследование от определенных классов. Это может быть полезно для создания строгих иерархий классов или ограничения неправильного использования вашего кода.
Регистрация классов или автоматическое создание экземпляров: Метаклассы могут автоматически регистрировать классы или создавать их экземпляры при определении класса. Это может быть полезно для создания плагинов, фабрик объектов или других паттернов проектирования, где вам нужно управлять классами и их экземплярами без явного указания от пользователя вашего кода.
Важно отметить, что метаклассы используются редко и чаще всего применяются в сложных и специфических случаях. В большинстве случаев рекомендуется использовать другие подходы, такие как декораторы, миксины или наследование, чтобы добиться желаемого поведения классов. Однако в определенных ситуациях метаклассы могут быть самым эффективным и элегантным решением."
1;;Когда будет выполнена ветка else в конструкции try…except…else?;Если код в try будет выполнен без ошибок.
1;;Опишите процесс компиляции в python.;"Python — это интерпретируемый язык, а это значит, что он не требует компиляции, как C или C++. Вместо этого интерпретатор Python читает и выполняет исходный код напрямую. Однако Python использует форму компиляции, называемую компиляцией байт-кода.

Когда сценарий Python запускается в первый раз, интерпретатор компилирует его в байтовый код, представляющий собой низкоуровневое представление исходного кода. Затем этот байт-код выполняется виртуальной машиной Python (PVM), которая представляет собой интерпретатор, который считывает байт-код и выполняет его.

Байт-код хранится в каталоге pycache с расширением .pyc. Python проверяет, есть ли у файла .py уже соответствующий файл .pyc, и, если файл .pyc старше файла .py, он компилирует файл .py в новый файл .pyc.

Таким образом, процесс «компиляции» в Python включает интерпретатор, который компилирует исходный код в байтовый код, который затем выполняется PVM. Однако этот процесс происходит автоматически и за кулисами, без необходимости пользователю явно вызывать отдельный шаг компиляции."
1;;Что такое интернирование строк? Почему это есть в python?;"Python оптимизирует работу со строками, используя метод интернирования, то есть для некоторых неизменяемых объектов python хранит только 1 экземпляр в памяти, как следствие, если в 2х переменных хранятся одинаковые строки, то они будут ссылаться на одну ячейку в памяти. 
При запуске программы интернирование происходит до момента её выполнения, именно поэтому в случае примера 2 строка, полученная при помощи конкатенации “hell” и “o” не была интернирована. Как следствие, во время выполнения программы строка “hello” (переменной a) и строка “hello” (полученная при помощи c + “o”) не будут ссылаться на один объект в памяти."
1;Как это работает;Опишите процесс работы с исключениями в Python. Как можно создать собственное исключение и обрабатывать разные типы исключений?;"Исключения в Python - это события, возникающие во время выполнения программы, которые обозначают ошибку или аномальное состояние. Обработка исключений позволяет гибко управлять ошибками и предотвращать аварийное завершение программы.

Вот основные конструкции для работы с исключениями в Python:

try: Блок кода, в котором может возникнуть исключение. Если в этом блоке возникает исключение, выполнение кода в блоке прекращается, и интерпретатор переходит к соответствующему блоку except.
except: Блок кода, который выполняется, если возникло определенное исключение в блоке try. Можно указать несколько блоков except для обработки различных типов исключений.
finally: Блок кода, который выполняется в любом случае после блока try, независимо от того, возникло ли исключение или нет.
Чтобы создать собственное исключение, нужно определить новый класс, унаследованный от базового класса Exception или одного из его подклассов. Затем можно вызывать это исключение с помощью оператора raise.

Пример работы с исключениями и создания собственного исключения:

class MyException(Exception):
pass
def test_function(value):
if value < 0:
raise MyException(""Отрицательное значение не допускается"")
try:
test_function(-1)
except MyException as e:
print(f""Обработка собственного исключения: {e}"")
except Exception as e:
print(f""Обработка общего исключения: {e}"")
finally:
print(""Этот блок выполняется в любом случае"")
В этом примере мы создаем собственное исключение MyException, унаследованное от базового класса Exception. Затем мы определяем функцию test_function, которая вызывает исключение MyException, если ее аргумент меньше нуля. В блоке try, мы вызываем функцию test_function с отрицательным значением и обрабатываем возникающие исключения с помощью блоков except. Блок finally выполняется после блока try, независимо от того, возникло исключение или нет."
5;Определение;Middleware;"Middleware в Django - это компонент, который обеспечивает обработку запросов и ответов, позволяет добавлять дополнительную функциональность и обрабатывать запросы и ответы до того, как они достигнут представления (view).

Middleware в Django выполняет следующие функции:

Обработка запросов: Middleware может выполнять действия перед обработкой запроса в представлении. Например, он может проверять авторизацию пользователя, проверять CSRF-токены, выполнять логирование запросов и другие операции, которые должны быть выполнены перед обработкой запроса приложением.

Обработка ответов: Middleware может также изменять или добавлять информацию в ответ, возвращаемый веб-приложением. Например, он может добавлять заголовки к ответу, устанавливать кэш или изменять содержимое ответа."
5;Определение;Migrations - что такое и как работает.;"Миграции в Django - это механизм, который обеспечивает автоматическое создание и обновление структуры базы данных в соответствии с определенными моделями (models) в Django приложении.

Миграции позволяют вам создавать, изменять и удалять таблицы, поля и связи между ними, сохраняя данные, уже содержащиеся в базе данных. При создании или изменении моделей Django генерирует миграционные файлы, которые описывают изменения, необходимые для синхронизации базы данных с определенными моделями. Каждый файл миграции содержит инструкции, такие как создание таблиц, добавление полей или изменение существующих полей."
5;Определение;Кратко расскажите об особенностях, преимуществах и недостатках Django.;"Фреймворк Django — мощный и многофункциональный комбайн. Он изначально включает в себя кучу полезных компонентов:
ORM (от англ. Object-Relational Mapping, «объектно-реляционное отображение»), встроенный административный интерфейс (или простым языком «админку»), шаблонизатор, библиотеку работы с формами, систему кеширования и интернализации, систему авторизации и аутентификации и многое другое.
Django — промышленный фреймворк. Он сделан так, чтобы код проекта легко читался и документировался. Это позволяет разработчикам лучше ориентироваться в чужом коде и переиспользовать уже готовые приложения в своих проектах. 
В небольших проектах или прототипах использование Django приводит к тому, что вместо нескольких строчек кода приходится создавать десятки дополнительных файлов и каталогов, без которых можно было бы обойтись"
4;Определение;Отличия TCP- и UDP-протоколов;"Являются сетевыми протоколами транспортного уровня и определяют правила взаимодействия устройств в сети, благодаря которым различные девайсы могут работать друг с другом, несмотря на конструктивные различия. 
У протокола TCP есть несколько особенностей:
- Система нумерации сегментов. TCP отслеживает передаваемые и принимаемые сегменты, присваивая номера каждому из них. Байтам данных, которые должны быть переданы, присваивается определенный номер байта, в то время как сегментам присваиваются порядковые номера.
- Управление потоком. Функция ограничивает скорость, с которой отправитель передает данные. Это делается для обеспечения надежности доставки, в том числе чтобы компьютер не генерировал пакетов больше, чем может принять другое устройство. Если говорить простым языком, то получатель постоянно сообщает отправителю о том, какой объем данных может быть получен.
- Контроль ошибок. Функция реализуется для повышения надежности путем проверки байтов на целостность.

Протокол TCP гарантирует доставку, а также обеспечивает целостность данных, передаваемых в сети. Поэтому он применяется для передачи данных, которые чувствительны к нарушению целостности, — например, текстов, файлов и т.п. Вот несколько протоколов, которые работают по TCP: 

SSH, FTP, Telnet: в данных протоколах TCP используется для обмена файлами.
SMTP, POP, IMAP: протоколы, где TCP отвечает за передачу сообщений электронной почты.
HTTP/HTTPS: протоколы, где TCP отвечает за загрузку страниц из интернета.

Если нам очень важна скорость передачи, а вот потеря пакетов не так критична (как, например, в голосовом или видеотрафике), то лучше использовать UDP, или User Datagram Protocol. В отличие от TCP он обеспечивает передачу данных без получения подтверждения от пользователя. Проще говоря, просто отправляет пакеты и не ждет ничего в ответ. Из-за этого достигается высокая скорость в ущерб надежности.

Чаще всего UDP применяется в чувствительных ко времени службах, где потерять пакеты лучше, чем ждать. Звонки в Skype или Google Meet, стриминг видео, онлайн-трансляции используют этот протокол из-за того, что они чувствительны ко времени и рассчитаны на определенный уровень потерь. Вся голосовая связь через интернет работает по протоколу UDP. Также UDP очень часто используется в онлайн-играх. Аналогичная история с DNS-серверами, поскольку они должны быть быстрыми и эффективными."
4;Определение;Назовите идемпотентные методы;"Идемпотентные методы - это методы в протоколе HTTP, которые можно вызывать несколько раз, и результат будет одинаковым, как если бы метод был вызван только один раз (состояние сервера или ресурсов не изменится после каждого повторного вызова.). 

Например, метод GET является идемпотентным, потому что повторный вызов не приведет к изменению состояния сервера или ресурсов. Вы можете вызывать метод GET несколько раз, и каждый раз получите одинаковый ответ.

Также идемпотентными являются методы DELETE и PUT. Если вы повторно отправите запрос DELETE на удаление ресурса или запрос PUT для обновления ресурса, сервер обрабатывает запрос только один раз, и дальнейшие повторные вызовы не будут иметь никакого эффекта.

Если нужна идемпотентность в других методах : 
- использовать токен идемпотентности для защиты от многократного создания заказов. При передаче запроса клиент предает токен идемпотентности, если сервер видит один и тот же токен второй раз, то не создает заказ, а отвечает ""заказ создан"".
- использование драфтов: неидемпотентный запрос (POST) создает черновик, а идемпотентный запрос (PUT) подтверждает создание заказа.

Идемпотентность методов HTTP важна, потому что она позволяет безопасно повторять запросы без нежелательных побочных эффектов или изменений состояния на сервере. Это упрощает разработку и поддержку систем, основанных на HTTP, и обеспечивает предсказуемость и надежность взаимодействия между клиентом и сервером."
4;Определение;Отличия методов http;"HTTP (Hypertext Transfer Protocol) - это протокол, используемый для передачи данных между клиентом и сервером в сети. В HTTP существуют различные методы (или запросы), которые определяют тип операции, выполняемой на ресурсе сервера:

GET:
Используется для получения данных с сервера.
Метод безопасен, то есть не должен изменять состояние сервера или ресурсов.

POST:
Используется для отправки данных на сервер для обработки.
Метод не безопасен, поскольку может изменить состояние сервера или ресурсов (например, создание новой записи на сервере).

PUT:
Используется для обновления или замены существующего ресурса на сервере.
Метод идемпотентный, то есть повторные запросы с одинаковыми данными не изменят состояние сервера.

DELETE:
Используется для удаления ресурса на сервере.
Также является идемпотентным методом.

PATCH:
Используется для частичного обновления ресурса на сервере.
Также является идемпотентным методом.

Идемпотентные методы - это методы в протоколе HTTP, которые можно вызывать несколько раз, и результат будет одинаковым, как если бы метод был вызван только один раз."
4;Определение;В чем отличие 401 и 403 ошибки?;"401 Unauthorized (Неавторизован) и 403 Forbidden (Запрещено) являются HTTP статусами, указывающими на ошибку доступа к ресурсу. 

Ошибка 401 Unauthorized означает, что клиент не предоставил аутентификационных сведений для доступа к запрашиваемому ресурсу.

Ошибка 403 Forbidden указывает на то, что доступ к запрашиваемому ресурсу запрещен, и клиент не имеет права на его использование. Это может быть связано с ограничениями прав доступа, настройками безопасности или другими ограничениями, установленными на сервере. "
7;Определение;Кратко расскажите об особенностях, преимуществах и недостатках FastAPI.;"Особенности FastAPI:

Автоматическая генерация документации: FastAPI автоматически генерирует интерактивную документацию API на основе аннотаций типов и комментариев к коду, что значительно упрощает разработку и поддержку API.
Поддержка асинхронности: FastAPI поддерживает асинхронное программирование с использованием синтаксиса async/await. Это позволяет обрабатывать запросы параллельно и повышает эффективность при работе с внешними сервисами или базами данных.
Встроенная валидация и сериализация данных: FastAPI предоставляет интеграцию с пакетом Pydantic для автоматической валидации входных данных, сериализации и десериализации моделей данных, что снижает количество ошибок и упрощает обработку данных.

Не подходит для всех случаев: FastAPI может быть излишним для простых веб-приложений или микросервисов с низкой нагрузкой. 
Относительная новизна: FastAPI является относительно новым фреймворком, поэтому у него может быть меньше комьюнити, чем у Django."
6;Как это работает;Как Flask обрабатывает запрос?;"Откуда Flask знает, какую функцию выводить, когда он получает запрос от клиента?

Когда веб-приложение на Flask получает запрос от клиента, Flask использует механизм маршрутизации для определения, какую функцию-обработчик вызвать для данного запроса. Маршрутизация основана на URL-адресе, указанном в запросе.

Маршрутизациясоздается с помощью декоратора route или метода add_url_rule() в экземпляре Flask. 
@app.route(&apos;/&apos;)
def index():
return &apos;Hello Flask&apos;

def index():
return &apos;Hello World&apos;

app.add_url_rule(&apos;/&apos;, &apos;index&apos;, index)

Получить доступ к этим соответствиям можно с помощью атрибута url_map у экземпляра Flask.
>>> app.url_map
Map([<Rule &apos;/&apos; (OPTIONS, GET, HEAD) -> index>,
<Rule &apos;/static/&apos; (OPTIONS, GET, HEAD) -> static>,
<Rule &apos;/books/&apos; (OPTIONS, GET, HEAD) -> books>,
<Rule &apos;/user/&apos; (OPTIONS, GET, HEAD) -> user_profile>])"
6;Определение;Сессии во Flask;
6;Определение;Куки во Flask;"Куки — это всего лишь фрагмент данных, которые сервер устанавливает в браузере. Вот как это работает:

- Браузер отправляет запрос на получение веб-страницы от сервера.
- Сервер отвечает на запрос, отправляя запрошенную страницу вместе с одним или несколькими куки.
- При получении ответа браузер рендерит страницу и сохраняет куки на компьютере пользователя.
- Последующий запрос на сервер будет включать информацию из куки в заголовке Cookie. Так будет продолжаться, пока не истечет сроки куки. Как только это происходит, куки удаляется из браузера.

Во Flask для настройки куки используется метод объекта ответа set_cookie(). Синтаксис set_cookie() следующий:
set_cookie(key, value="""", max_age=None)
Для доступа к куки используется атрибут cookie объекта request. cookie — это атрибут типа словарь, содержащий все куки, отправленные браузером. 
Чтобы удалить куки, нужно вызвать метод set_cookie() с названием куки, любым значением и указать срок max_age=0. "
6;Определение;Контексты во Flask;"Согласно документации Flask существует два вида контекстов: 
- Контекст приложения
- Контекст запроса

Контекст приложения используется для хранения общих переменных приложения, таких как подключение к базе данных, настройки и т. д. Контекст приложения может быть и не создан, если фласк видит, что мы не собираемся использовать его в обработчике. Создается до контескта запроса. А контекст запроса(создается всегда) используется для хранения переменных конкретного запроса.

С контекстом приложения связаны две глобальные переменные: 
- g используется, чтобы временно хранить данные во время обработки запроса. Данные в g сбрасываются после каждого запроса
- current_app ссылается на экземпляр, который обрабатывает запрос

С контекстом запроса связаны две переменные:
- request содержит данные, связанные с текущим запросом, доступна в пределах текущего запроса
- session сохраняется между запросами, уникальна для источника запроса. Может использоваться для авторизации на сайте

Чтобы получить доступ к объектам, предоставляемым контекстами приложения и запроса вне функции представления, нужно сперва создать соответствующий контекст. Создать контекст приложения можно с помощью метода app_context() для экземпляра Flask.
>>> from main2 import app
>>> from flask import request, current_app
>>>
>>> with app.app_context():
... current_app.name #&apos;main2&apos;

Похожим образом можно создавать контекст запроса с помощью метода test_request_context() в экземпляре Flask.
>>> with app.test_request_context(&apos;/products&apos;):
... request.path # получим полный путь к запрашиваемой странице(без домена). &apos;/products&apos;
... request.method #&apos;GET&apos;
... current_app.name #&apos;main2&apos;"
6;Определение;Кратко расскажите об особенностях, преимуществах и недостатках Flask.;"У этого фреймворка небольшой размер исходной кодовой базы, поэтому его называют микрофреймворком. По умолчанию он включает в себя только обработчик запросов и шаблонизатор, а простейшее приложение на Flask может состоять всего из нескольких строк. Разработчики этого фреймворка осознанно хотели сохранить ядро простым, но расширяемым.
С помощью Flask можно реализовать практически любую задачу: от простого одностраничного сайта до серьёзного проекта с авторизацией, аутентификацией и другими возможностями. Flask подходит для задач, которые подразумевают гибкость в выборе компонентов. Разработчик сам принимает решение, что ему пригодится в работе. 
У Flask нет встроенной ORM, но фреймворк совместим со сторонними проектами — SQLAlchemy, PonyORM, Tortoise ORM и другими. Наиболее популярный среди них — SQLAlchemy.
Во Flask есть интерактивная оболочка.Это по сути обычный интерпретатор Python, который запускается в режиме командной строки и сразу же после запуска импортирует экземпляр приложения Flask."
2;Определение;Что такое индексы в БД и для чего нужны. ;"Поиск данных в большой таблице БД может занимать много времени. Для решения этой проблемы были придуманы индексы, своего рода справочники, предметные указатели или оглавления. Система прочитывает таблицу и составляет указатель, в котором хранится информация, где искать нужную запись. Индексы создаются не для таблицы целиком, а для столбцов таблиц. 
Добавление индексов существенно увеличивает «стоимость» добавления новых записей в базу: после добавления или изменения записей надо будет обновить индекс. Процесс ресурсоёмкий, но это разумная плата за быстродействие системы. Чтение данных в приоритете, ведь посетители чаще читают, чем пишут, и выгоднее редко тратить ресурсы на составление индекса, чем часто — на длительный поиск по БД.
Для оптимизации базы данных надо проанализировать, как происходит поиск записей в моделях. Обычно это обращение по первичному ключу и фильтрация по полям, где указаны связи между таблицами. Некоторые базы автоматически добавляют индексы для внешних ключей. Но так происходит не всегда, и лучше указывать явно, что для этих колонок надо создать индексы. Также есть смысл добавить индексы для полей, по которым часто проводится поиск или сортировка.
В Django за добавление индекса отвечает аргумент db_index в свойствах модели. Для ForeignKey это поле по умолчанию имеет значение True. 
Поля модели, для которых указан параметр unique = True, тоже имеют индекс по умолчанию: он нужен при проверке уникальности переданного поля для работы самой базы данных."
2;SQL;Какие виды Join‘ов есть в SQL;"По стандарту SQL92 принято отделять фильтрацию от условий соединения таблиц с помощью оператора JOIN (англ. «соединение»).
- Внутреннее пересечение: INNER JOIN
Оператор INNER JOIN включает в результирующую таблицу только те записи, в которых выполняется условие, заданное в ON
- Левое внешнее соединение: LEFT OUTER JOIN
При обработке запроса LEFT OUTER JOIN объединяемые таблицы условно называют «левая» и «правая». «Левая» — та, которая вызвана в блоке FROM, «правая» — та, что указана после ключевого слова JOIN. «Правых» таблиц может быть и несколько.
В этом запросе OUTER — не обязательное слово. Можно использовать сокращённую запись: LEFT JOIN.
При выполнении запросов с LEFT JOIN возвращаются все строки левой таблицы. Данными из правой таблицы дополняются только те строки левой таблицы, для которых выполняются условия соединения, описанные после оператора ON. Для недостающих данных вместо строк правой таблицы вставляются NULL-значения.
- RIGHT OUTER JOIN
RIGHT JOIN – это такое же объединение, как и LEFT JOIN, но выводятся все записи из правой таблицы, а к ним добавляются только те данные из левой таблицы, в которых есть ключ объединения. В SQLite при попытке выполнить RIGHT JOIN возникнет ошибка: эта СУБД не поддерживает такие запросы.Выход есть: можно применить «разрешённый» в SQLite LEFT JOIN, поменяв таблицы местами. 
- FULL OUTER JOIN
При запросе FULL (OUTER) JOIN выводятся все записи из объединяемых таблиц. Те записи, у которых запрошенные значения совпадают — выводятся парами, у остальных недостающее значение заменяется на NULL (Python выведет None).
Другими словами, FULL JOIN == LEFT JOIN + RIGHT JOIN. SQLite не поддерживает оператор FULL JOIN. В SQLite такую задачу можно решить так: выполнить два запроса LEFT JOIN и объединить их через команду UNION — она объединяет данные из нескольких результирующих таблиц в одну. 
- CROSS JOIN
Объединение таблиц через CROSS JOIN возвращает декартово произведение таблиц — каждая запись левой таблицы объединится с каждой записью правой. Параметр ON при запросах CROSS JOIN не применяется."
2;Определение;Уровни изоляции транзакций;"Уровни изоляции в базах данных определяют уровень видимости и взаимодействия транзакций, выполняющихся параллельно. Вот некоторые основные уровни изоляции:

Уровень изоляции READ UNCOMMITTED: Это наименьший уровень изоляции. Он позволяет транзакции видеть неподтвержденные изменения, сделанные другими транзакциями. Это может привести к проблемам ""грязного чтения"", когда транзакция видит данные, которые могут быть откатаны или изменены другой транзакцией.

Уровень изоляции READ COMMITTED: Этот уровень изоляции гарантирует, что транзакции видят только подтвержденные изменения других транзакций. Это означает, что транзакция не будет видеть неподтвержденные изменения других транзакций. Однако могут возникнуть проблемы ""неповторяемого чтения"", когда одна и та же транзакция выполняет чтение дважды и видит разные значения из-за изменений, сделанных другими транзакциями.

Уровень изоляции REPEATABLE READ: Этот уровень изоляции гарантирует, что одна и та же транзакция будет видеть одни и те же значения данных в течение всей транзакции. Другие транзакции не могут внести изменения в данные, которые видны для выполняющейся транзакции. Однако могут возникнуть проблемы ""фантомного чтения"", когда одна и та же транзакция выполняет чтение дважды и видит разные наборы строк из-за добавления или удаления других транзакций.

Уровень изоляции SERIALIZABLE: Это самый высокий уровень изоляции. Он гарантирует, что транзакции выполняются последовательно, как если бы они были одиночными операциями. Одновременные операции между транзакциями не допускаются, и это предотвращает проблемы ""неповторяемого чтения"" и ""фантомного чтения"". Однако это может привести к более низкой производительности из-за ограничения параллельности.
Уровень изоляции указывается на уровне транзакции. При начале транзакции в базе данных можно указать желаемый уровень изоляции. В большинстве баз данных, включая PostgreSQL, это делается с помощью команды SET TRANSACTION ISOLATION LEVEL."
2;SQL;Транзакции. Рассказать, привести пример.;Транзакция — это набор операций по работе с базой данных (БД), объединенных в одну атомарную пачку.
2;SQL;Нормальные формы в базах данных, сколько, в чем смысл, бывают ли случаи когда выгоднее не нормализовать связи.;"Процесс проектирования БД с использование метода НФ является итерационным и заключается в последовательном переводе отношения из 1НФ в НФ более высокого порядка по определенным правилам. 
Первая НФ - в таблице нет повторяющихся строк, в каждой ячейке находится только одно значение.
Вторая НФ - есть первичный ключ и все атрибуты зависят от первичного ключа целиком, а не от его части.
Третья НФ - все атрибуты зависят от первичного ключа, а не от других атрибутов
Нормальная форма Бойса-Кодда - "
2;SQL;Разница Explain и Explain Analyze;"Пример команды EXPLAIN:
EXPLAIN SELECT * FROM customers WHERE age > 30;
Seq Scan on customers (cost=0.00..10.50 rows=3 width=64)
Filter: (age > 30)
Здесь показан план выполнения запроса. Seq Scan указывает на последовательное сканирование таблицы customers. cost представляет оценку стоимости выполнения операции, а rows показывает оценочное количество возвращаемых строк.

Пример команды EXPLAIN ANALYZE:
EXPLAIN ANALYZE SELECT * FROM customers WHERE age > 30;
Результат команды EXPLAIN ANALYZE может выглядеть примерно так:
Seq Scan on customers (cost=0.00..10.50 rows=3 width=64) (actual time=0.010..0.020 rows=5 loops=1)
Filter: (age > 30)
Rows Removed by Filter: 95
Planning Time: 0.066 ms
Execution Time: 0.039 ms

Здесь, помимо информации о плане выполнения запроса, также представлены фактические временные метрики выполнения. actual time показывает фактическое время выполнения операции, loops указывает количество выполнений операции. Rows Removed by Filter показывает количество строк, исключенных после фильтрации.

Команда EXPLAIN ANALYZE выполняет сам запрос и возвращает точные метрики производительности. Это полезно для более точного анализа и оптимизации запросов, так как можно видеть фактическое время выполнения, количество возвращаемых и исключенных строк и другие статистические данные."
2;Определение;Primary key, Foreign key;Primary key - автоинкрементный (тот, который каждый раз при создании записи автоматически увеличивается на единицу) первичный ключ. Уникален для каждой записи в рамках таблицы. Может быть составным.
2;Определение;Что такое ORM, где она используется и можно ли без нее обойтись?;"«Объектно-реляционное отображение» и означает «технология программирования, которая связывает базы данных с концепциями объектно-ориентированных языков программирования»… т.е. ORM — прослойка между базой данных и кодом который пишет программист, которая позволяет созданые в программе объекты складывать/получать в/из бд.

Плюсы использования ORM :
Упрощенный доступ к данным: ORM скрывает сложности работы с SQL и базами данных, предоставляя более простой и понятный интерфейс для работы с данными.
Переносимость кода: Использование ORM позволяет писать код, который не зависит от конкретной СУБД, что упрощает перенос приложения на другую базу данных.

Когда можно обойтись без ORM?
Если ваше приложение имеет простую структуру данных
В некоторых случаях, особенно когда требуется максимальная производительность или определенные возможности базы данных, напрямую писать SQL-запросы может быть предпочтительным вариантом."
2;Определение;Что такое сессия, особенности работы с сессией.;Сессия — последовательность действий с момента последнего коммита. Её можно либо подтвердить, то есть сохранить изменения в базе данных (commit), либо отменить до текущего состояния в базе данных (rollback). Если при сохранении возникнет ошибка, все изменения сделанные с момента последнего коммита отменятся.
2;Определение;Что такое нереляционные БД?;"Нереляционные базы данных, также известные как NoSQL (Not Only SQL), предлагают ряд преимуществ по сравнению с традиционными реляционными базами данных. Рассмотрим особенности и преимущества некоторых популярных типов нереляционных баз данных:

Документоориентированные базы данных (Document-oriented databases):
Пример: MongoDB.
Особенности: Хранение данных в формате документов, обычно в JSON-подобной структуре. Гибкость схемы данных позволяет хранить и обрабатывать полиморфные данные без строгой предварительной схемы. Поддержка вложенных документов облегчает представление сложных объектов. Простота горизонтального масштабирования и возможность использования гибкого языка запросов для поиска и фильтрации данных.

Ключ-значение базы данных (Key-value databases):
Примеры: Redis, Amazon DynamoDB.
Особенности: Простейшая структура данных, где каждое значение связано с уникальным ключом. Гибкость и высокая производительность чтения и записи. Поддержка кэширования, сессий и хранения временных данных. Широкое применение для хранения данных с быстрым доступом, например, для кэширования, учетных записей пользователей и хранения сеансов.

Семейство столбцовых баз данных (Columnar databases):
Примеры: Apache Cassandra, Amazon DynamoDB, Apache HBase.
Особенности: Хранение данных в виде столбцов, вместо строк. Гибкость схемы и возможность хранения разнородных данных. Высокая производительность при операциях чтения и записи в большом масштабе. Устойчивость к отказам и возможность горизонтального масштабирования на несколько узлов. Подходят для больших объемов данных, временных рядов, аналитических и OLAP-запросов.

Графовые базы данных (Graph databases):
Примеры: Neo4j, Amazon Neptune.
Особенности: Хранение данных в виде графа, где узлы представляют сущности, а ребра - их связи. Эффективные операции для поиска, обхода и анализа связей между данными. Удобство работы с семантическими связями и сложными структурами данных, такими как социальные сети, рекомендательные системы и графовые алгоритмы."
2;Как это работает;Масштабирование нереляционных баз данных;"Масштабирование нереляционных баз данных на несколько серверов или узлов может быть достигнуто с использованием различных подходов, таких как репликация, шардинг или комбинация обоих. Вот примеры этих подходов:

Репликация:

Мастер-слейв репликация: Один сервер (мастер) принимает записи и реплицирует их на несколько серверов-слейвов. Это позволяет распределить чтение по разным серверам и обеспечить отказоустойчивость.
Мастер-мастер репликация: Несколько серверов работают в режиме мастера, и каждый из них может принимать записи. Изменения синхронизируются между мастерами. Это обеспечивает более высокую отказоустойчивость и возможность записи на любой из мастеров.
Шардинг (горизонтальное масштабирование):

Горизонтальное разделение: Данные разделяются на несколько фрагментов (шардов) на разных серверах. Каждый сервер хранит только часть данных. Это позволяет равномерно распределить нагрузку и увеличить общую пропускную способность системы.
Хэш-шардинг: Данные распределяются между серверами с использованием хэш-функций. Это обеспечивает случайное распределение данных, но может затруднить запросы, требующие объединения данных из разных шардов.
Региональное шардинг: Данные разделяются на основе регионов или логических разделов, например, по географическому местоположению или типу данных. Это позволяет лучше управлять запросами и доступом к данным.
Комбинированный подход:

Комбинированный подход может включать в себя использование репликации и шардинга вместе. Например, можно настроить репликацию между мастером и несколькими слейвами, а затем применить шардинг к каждому из слейвов для более гибкого и эффективного масштабирования.
Примеры конкретных решений для масштабирования нереляционных баз данных включают Apache Cassandra (шардинг), MongoDB (репликация и шардинг), и Redis (репликация). Эти базы данных предоставляют механизмы для настройки и управления масштабированием на несколько серверов или узлов."
2;Как это работает;В чем преимумщества нереляционной БД.;"Нереляционные базы данных, также известные как NoSQL (Not Only SQL) базы данных, предлагают ряд преимуществ по сравнению с традиционными реляционными базами данных. Вот некоторые из основных преимуществ нереляционных баз данных:

Гибкость в модели данных: Нереляционные базы данных предлагают гибкую модель данных, которая позволяет хранить и обрабатывать разнородные и полиморфные данные. Они позволяют хранить данные в форматах, таких как документы, ключ-значение, столбцы или графы, в зависимости от потребностей приложения. Это особенно полезно для хранения и обработки неструктурированных данных, таких как JSON-документы или иерархические данные.

Масштабируемость и производительность: Нереляционные базы данных обеспечивают горизонтальную масштабируемость, что означает, что они могут легко масштабироваться на несколько серверов или узлов. Это позволяет обрабатывать большие объемы данных и обеспечивать высокую производительность даже при большой нагрузке. Некоторые нереляционные базы данных также предлагают репликацию и шардинг для обеспечения отказоустойчивости и более эффективного использования ресурсов.

Более простая модель данных и запросов: В нереляционных базах данных модель данных обычно более проста и плоская по сравнению с реляционными базами данных. Запросы также могут быть проще и более интуитивными для разработчиков. Это может упростить разработку и ускорить время развертывания приложений.

Работа с неструктурированными данными: Нереляционные базы данных хорошо подходят для работы с неструктурированными данными, такими как тексты, мультимедийные файлы или временные ряды. Они позволяют хранить и обрабатывать такие данные без необходимости строгой схемы или предварительного определения структуры."
1;Как это работает;Как ускорить существующий код python?;"Чтобы ускорить существующий код на Python, можно использовать несколько подходов:

Векторизация: векторизация позволяет оптимизировать код, который выполняет большое количество операций над массивами данных, например, использование библиотеки NumPy.

Выбор правильных структур данных: выбор правильных структур данных и алгоритмов может значительно ускорить выполнение кода. Например, использование словарей может быть более эффективным, чем использование списков.

Компиляция: компиляция Python-кода в байт-код или в машинный код может ускорить выполнение кода. Для этого можно использовать Cython, Nuitka или PyPy.

Многопоточность: использование многопоточности может ускорить выполнение задач, которые можно разделить на несколько независимых частей.

Параллелизм: параллельное выполнение задач на нескольких ядрах процессора может ускорить выполнение кода.

Оптимизация: такие инструменты, как cProfile и line_profiler, могут помочь оптимизировать код, выявляя узкие места в его выполнении и предоставляя информацию о времени выполнения каждой строки кода.

Компромиссы: если выполнение кода нельзя ускорить до приемлемого уровня, можно рассмотреть возможность использования компромиссов, например, уменьшить количество данных, обрабатываемых кодом, или упростить логику выполнения задачи."
1;Определение;Что такое виртуальное окружение?;"Виртуальное окружение - это механизм, который позволяет создавать изолированные окружения для установки и использования пакетов Python. Это полезно, когда вам нужно установить определенную версию пакета или когда вам нужно иметь одновременный доступ к разным версиям библиотек в зависимости от проекта.

Создание виртуального окружения позволяет изолировать зависимости проекта от системных зависимостей и других проектов, работающих на той же машине. Это помогает избежать конфликтов зависимостей, что может привести к ошибкам и сбоям.

Вы можете создать виртуальное окружение Python с помощью модуля venv, который поставляется в стандартной библиотеке Python. Например, вы можете создать виртуальное окружение в текущей директории, выполнитив следующую команду в терминале:

python3 -m venv myenv"
1;Определение;Python — это императивный или декларативный язык?;"Python является императивным языком программирования. В императивном программировании программист составляет последовательность команд, которые выполняются компьютером. Python также поддерживает некоторые функциональные и объектно-ориентированные концепции программирования, однако основной подход в языке является императивный.

""Императивный язык"" это термин, который относится к классу языков программирования, использующих прямые команды для управления компьютером, в отличие от декларативных языков. В императивных языках программист явно описывает действия, которые нужно выполнить компьютеру, а не просто описывает желаемый результат. Примеры императивных языков программирования это Java, C, C++, Python и JavaScript.

Декларативный язык - это язык программирования, который назначает техническую реализацию системы или программы для достижения определенной цели, но не указывает конкретных шагов для ее выполнения. Вместо этого вы определяете, какая информация должна быть обработана, а система сама определяет, как решить эту проблему. Примерами декларативных языков являются SQL для работы с базами данных и HTML для создания веб-страниц. Такие языки обычно используются в случаях, когда важнее задать желаемый результат, чем указать, как добиться этого результата."
1;Определение;Что такое менеджер пакетов? Какие менеджеры пакетов вы знаете?;"Менеджер пакетов - это инструмент, который позволяет управлять установкой, обновлением и удалением библиотек и зависимостей в проектах на языке Python. Некоторые из наиболее популярных менеджеров пакетов Python:

pip - это стандартный менеджер пакетов Python. Он позволяет устанавливать пакеты из Python Package Index (PyPI) и других источников, а также управлять зависимостями проекта.

conda - это менеджер пакетов и среда управления, который позволяет управлять пакетами и зависимостями для проектов на Python, а также для других языков программирования и платформ.

easy_install - инструмент для установки и управления пакетами Python, который был стандартным до выпуска Python 3. Используется редко в настоящее время.

poetry - новый менеджер пакетов, предназначенный для замены в некоторой степени pip и virtualenv."
1;Определение;В чём преимущества массивов numpy по сравнению с (вложенными) списками python?;"Основное преимущество массивов NumPy перед списками Python заключается в том, что NumPy использует более оптимизированную память и имеет более эффективные методы работы с массивами, что делает его подходящим выбором для работы с большими объемами данных и научных вычислений. Например, с NumPy вы можете выполнять бродкастинг (broadcasting), матричные операции и другие векторизованные вычисления с более высокой производительностью, чем при использовании вложенных списков.

Некоторые из основных преимуществ NumPy:

Более оптимизированная память, что позволяет NumPy работать быстрее с большим объемом данных

Встроенные методы для выполнения арифметических операций, таких как сумма и произведение, которые могут работать сразу над всеми элементами массивов.

Возможность выполнять матричные операции и другие векторизованные вычисления.

Простой синтаксис для выполнения операций над массивами.

Возможность конвертировать массивы NumPy в другие формы данных, такие как списки Python или таблицы Pandas.

Eсли вы работаете с массивами данных, над которыми нужно выполнять научные вычисления, то использование NumPy будет более предпочтительным вариантом, чем использование списков Python."
1;Определение;Какие функции из collections и itertools вы используете?;"В модулях collections и itertools в Python есть множество полезных функций, которые могут использоваться в различных задачах. Некоторые из наиболее часто используемых функций включают:

defaultdict: это удобный способ создания словаря с заданным значением по умолчанию для любого ключа, который еще не был добавлен в словарь.

Counter: это удобный способ подсчета количества встречаемых элементов в списке или другом итерируемом объекте. Он возвращает объект, который можно использовать как словарь, где ключами являются элементы, а значения - количество их вхождений.

namedtuple: можно создать именованный кортеж с заданными полями, что может быть удобно для работы с данными, которые имеют структуру, но не требуют создания класса.

itertools.chain: позволяет конкатенировать несколько итерируемых объектов в единый итератор.

itertools.groupby: позволяет группировать элементы итерируемого объекта по заданному ключу.

itertools.combinations и itertools.permutations: генерируют все различные комбинации или перестановки элементов из заданного множества."
1;Определение;Что делает флаг PYTHONOPTIMIZE?;"Флаг -O или PYTHONOPTIMIZE в Python используется для оптимизации скомпилированного кода, что может привести к ускорению выполнения программы. Этот флаг удаляет отладочную информацию, отключает asset checks, asserts и отладочные проверки.

Стандартная оптимизация -O удаляет docstrings из скомпилированного byte-code, а также удаляет assert statements. С флагом -OO удаляются все docstrings в модулю (включая те, которые не соответствуют многострочным строкам) и также удаляются assert statements.

Запуск интерпретатора Python с флагом -O может уменьшить размер скомпилированного кода и сократить потребление памяти, что может привести к ускорению работы программы. Однако, для большинства приложений, эта оптимизация может не иметь значимого влияния на производительность.

Например, для запуска скрипта с флагом -O, можно использовать следующую команду в командной строке:

python -O my_script.py"
